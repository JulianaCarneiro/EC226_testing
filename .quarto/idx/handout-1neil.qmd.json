{"title":"Classical Linear Regression Model & Ordinary Least Squares","markdown":{"yaml":{"title":"Classical Linear Regression Model & Ordinary Least Squares","format":{"html":{"toc-depth":3,"number-sections":true,"resources":["handout-1.pdf"],"code-fold":true},"pdf":{"toc":true,"number-sections":true,"keep-md":false,"output-file":"handout-1.pdf","echo":false}}},"headingText":"Overview","containsRefs":false,"markdown":"\n\n\nIn this handout we will revisit the Classical Linear Regression Model (CLRM) [see @wooldridge2010, chap. 1-2]. The goal of this week's lecture is to:\n\n1.  understand the model specification;\n\n2.  it's underlying assumptions;\n\n3.  and the appropriate interpretation;\n\n4.  the OLS estimator, using linear algebra;\n\n5.  the geometry of OLS and partitioned regression result.\n\n## Model Specification\n\nThe linear population regression model is given by,\n\n$$ \n\\begin{aligned}\n  Y_i =& X_i'\\beta+\\varepsilon_i \\\\\n  =& \\beta_1\\mathbf{1}+\\beta_2X_{i2}+\\beta_3X_{i3}+...+\\beta_kX_{ik}+\\varepsilon_i\n\\end{aligned}\n$$\n\nfor $i = 1,2,...,n$. Where,\n\n-   $i$: unit of observation; e.g. individual, firm, union, political party, etc.\n\n-   $Y_i \\in \\mathbb{R}$: scalar random variable.\n\n-   $X_i \\in \\mathbb{R}^k$: $k$-dimensional (column[^1]) vector of regressors, with $k<n$.[^2]\n\n-   $\\beta$: $k$-dimensional, non-random vector of unknown population parameters.\n\n-   $\\varepsilon_i$: *unobserved*, random error term.[^3]\n\n[^1]: My notation assumes that $X_i$ is a column vector, which makes $X_i'\\beta$ a scalar. Wooldridge (2010) uses the notation $X_i\\beta$, implying that $X_i$ is a row vector. This is a matter of preference.\n\n[^2]: You might also refer to the vector of regressors or explanatory variables. The terms covariates or control variables are more common in Microeconometrics literature, where regressors are typically included for identification of causal effects. Some texts will use the term independent variables, but this name implies a specific relationship between $Y$ and $X$ that need not hold. Note, we will asssume in this term that $n>k$; i.e. this is \"small\" data.\n\n[^3]: This is **NOT** the residual.\n\nThe linear population regression equation is **linear in parameters**. This is an important assumption that does NOT restrict the model from being non-linear in regressors. For example, the equation\n\n$$Y_i = \\beta_1 + \\beta_2X_{i2} + \\beta_3X_{i2}^{\\color{red}{2}} + \\varepsilon_i$$ non-linear in $X_{i2}$, but still linear in parameters. In contrast, the equation\n\n$$Y_i = \\beta_1 + \\beta_2X_{i2} + ({\\color{red}{\\beta_2\\beta_3}})X_{i3} + \\varepsilon_i$$ is non-linear in parameters.\n\n### Intercept\n\nThe constant (intercept) in the equation serves an important purpose. While there is no *a priori* reason for the model to have a constant term, it does ensure that the error term is mean zero.\n\n::: proof\nSuppose $E[\\varepsilon_i] = \\gamma$.\n\nWe can then define a new error term, $\\upsilon_i = \\varepsilon_i - \\gamma$, such $E[\\upsilon_i] = 0$. The population regression model can be rewritten as, $$ \\begin{aligned} Y_i =& X_ i'\\beta + v_i + \\gamma  \\\\\n=& \\underbrace{(\\beta_1+\\gamma)}_{\\tilde{\\beta}_1}\\mathbf{1} + \\beta_2X_{i2} + \\beta_3X_{i3} + ... + \\beta_kX_{ik} + v_i\n\\end{aligned}\n$$ The model has a new intercept $\\tilde{\\beta}_1=\\beta_1 + \\gamma$, but the other parameters remain unchanged.\n:::\n\n### Matrix notation\n\nFor a sample of $n$ observations, we can stack the unit-level linear regression equation into a vector,\n\n$$\nY =\\underbrace{\\begin{bmatrix}Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}}_{n\\times 1} = \\underbrace{\\begin{bmatrix}X_1'\\beta \\\\ X_2'\\beta \\\\ \\vdots \\\\ X_n'\\beta\\end{bmatrix}}_{n\\times 1}  + \\underbrace{\\begin{bmatrix}\\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n\\end{bmatrix}}_{n\\times 1}  \n= \\underbrace{\\begin{bmatrix}X_{11} & X_{12} & \\cdots & X_{1k}\\\\ X_{21} & X_{22} && \\\\ \\vdots & & \\ddots & \\\\ X_{n1} & & & X_{nk} \\end{bmatrix}}_{n\\times k} \\underbrace{\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_k\\end{bmatrix}}_{k\\times 1} + \\begin{bmatrix}\\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n\\end{bmatrix} = X\\beta + \\varepsilon\n$$ Notice, in matrix notation, you lose the transpose from $X_i'\\beta$. Apart from the absence of the $i$ subscript, this is a useful way of knowing the dimension of the equation (in my notes). You MUST always write $X\\beta$ and not $\\beta X$. For the scalar case, $X_i'\\beta = \\beta'X_i$, but for the vector case $\\beta X$ is not defined since $\\beta$ is $k\\times 1$ and $X$ is $n\\times k$.\n\n## CLRM Assumptions\n\n**Assumption CLRM 1.** Population regression equation is linear in parameters: $$Y = X\\beta+\\varepsilon$$\n\n**Assumption CLRM 2.** Conditional mean independence of the error term: $$E[\\varepsilon|X]=0$$\n\nAssumption CLRM 2. is stronger than $E[\\varepsilon_i|X_i]$ (mean independence for unit $i$). If all units were independent, then $E[\\varepsilon_i|X_i]$ would imply $E[\\varepsilon|X]=0$. However, since we have not (yet) assumed this, we need this stronger exogeneity assumption. Consider, if $i$ represented units of time ($t$), as in time-series models, independence across $i$ will not hold.\n\nTogether, CLRM 1. and CLRM 2. imply that\n\n$$ E[Y|X] = X\\beta  $$ This means that the Conditional Expectation Function is known and linear in parameters.\n\nConditional mean independence implies - by the Law of Iterated Expectations - mean independence of the error term,\n\n$$E[\\varepsilon|X]=0 \\Rightarrow E\\big[E[\\varepsilon|X]\\big]=E[\\varepsilon]=0$$\n\nand uncorrelatedness,\n\n$$E[\\varepsilon|X]=0 \\Rightarrow E[\\varepsilon X]=0$$ Note, neither of the above statements hold the other way around. Mean independence does not imply conditional mean independence and uncorrelatedness (zero correlation/covariance) does not imply conditional mean independence.\n\nUncorrelatedness rules out linear relationships between the regressors and error term while conditional mean independence rules out non-linear relationships too.\n\nIn general, distributional independence implies mean independence which then implies uncorrelatedness.\n\n::: {#nte-normality}\nIn the case joint-normally distributed random variables, uncorrelatedness implies independence. That is, if $$ \\begin{bmatrix}Y_1 \\\\ Y_2\\end{bmatrix}\\sim N\\bigg(\\begin{bmatrix}\\mu_1 \\\\ \\mu_2\\end{bmatrix},\\begin{bmatrix}\\sigma_1^2 & \\sigma_{12}\\\\ \\sigma_{21} & \\sigma_2^2\\end{bmatrix}\\bigg)$$ Then $\\sigma_{12}=\\sigma_{21}=0 \\iff f_1*f_2 = f_{12}$.\n:::\n\nWe will later show that uncorrelatedness is sufficient for consistency of the Ordinary Least Squares estimater, while conditional mean independence is required for unbiasedness of OLS.\n\n**Assumption CLRM 3.** Homoskedasticity: $Var(\\varepsilon|X) = E[\\varepsilon\\varepsilon'|X] = \\sigma^2I_n$\n\nCLRM 3. states that the variance of the error term is independent of $X$ and constant across units. The diagonal nature of the covariance matrix also implies that the error terms are uncorrelated across units in the data. Note, this does not imply independence of the error terms across units.\n\nModels with heteroskedasticity relax the assumption of constant variance, allowing for a richer variance-covariance matrix that typically depends on $X$.\n\nThis assumption is unlikely to hold in time-series models where units represent repeated observations across time. Such violations are referred to as serial correlation or autocorrelation.\n\nEven in cross-sectional data settings, you can have non-zero correlations across units in the data. A common instance of this is the case of clustering. Clustering can occur when units experience common/correlated 'shocks'; for example, the data contains groups of students from the same classroom who have a the same teacher. This can also be the result of clustered sampling, a common practice in multi-stage survey design.\n\n**Assumption CLRM 4.** Full rank: $rank(X)=k\\quad a.s.$ a.s.[^4]\n\n[^4]: See extra material on Linear Algebra to read more on rank.\n\nSince $X$ is a random variable we should add to the assumption: $rank(X) = k$ *almost surely* (abbreviated a.s.). This means that the set of events in which $X$ is not full rank occur with probability 0. The reason for this addition is that such a set of events (in the sample space) may not be empty.\n\nCLRM 4. is some time referred to as the absence of perfect (or exact) collinearity. Do not confuse this with multicollinearity. Multicollinearity occurs when regressors are highly (linearly) correlated with one another, yielding imprecise estimates.\n\n**Assumption CLRM 5.** Normality of the error term: $\\varepsilon|X \\sim N(0,\\sigma^2 I_n)$\n\n**Assumption CLRM 6.** Observations $\\{(Y_i,X_i): i=1,...,n\\}$ are independently and identically distributed (iid).\n\nCLRM 5 & 6 are not part of the Classical assumptions, but do simplify the problem of inference. Note, CLRM 5 implies independence across error terms, not implied by CLRM 3.\n\n### Non-random $X$\n\nThere is an alternative version of the CLRM in which $X$ is a non-random, matrix of regressors/predictors. With $X$ fixed, the error term is the only only random variable in the model. CLRM assumptions 1 and 4 remain the same, while CLRM 2, 3, 5, and 6 become:\n\n**Assumption CLRM 2^a^.** Mean independence of the error term: $$E[\\varepsilon]=0$$\n\n**Assumption CLRM 3^a^.** Homoskedasticity: $Var(\\varepsilon) = \\sigma^2I_n$\n\n**Assumption CLRM 5^a^.** Normality of the error term: $\\varepsilon \\sim N(0,\\sigma^2 I_n)$\n\n**Assumption CLRM 6^a^.** Observations $\\{\\varepsilon_i: i=1,...,n\\}$ are independently and identically distributed (iid).\n\n### Identification\n\nCLRM 1,2 and 4. are the *identifying* assumptions of the model. These assumptions allow us to write the parameter of interest as a set of 'observable' moments in the data. We can demonstrate this as follows.\n\n::: proof\nStart with CLRM 2.\n\n$$\n    E[\\varepsilon_i|X_i]=0 \n$$\n\nPre-multiply by the vector $X_i$, $$\n        X_iE[\\varepsilon_i|X_i]=0 \n$$ Since the expectation is conditional on $X_i$, we can bring $X_i$ inside the expectation function,\n\n$$\n        E[X_i\\varepsilon_i|X_i]=0 \n    $$ This conditional expectation is a random-function of $X_i$. If we take the expectation of this function w.r.t. $X$, we achieve the aforementioned result that conditional mean independence implies zero covariance, $$\n        E\\left[E[X_i\\varepsilon_i|X_i]\\right]=E[X_i\\varepsilon_i]=0 \n    $$\n\nNow substitute in for $\\varepsilon_i$ using the linear regression model from CLRM 1 and separate the resulting two terms,\n\n$$\n\\begin{aligned}\n    &E[X_i(Y_i-X_i'\\beta)]=0 \\\\\n    \\Rightarrow &E[X_iX_i']\\beta=E[X_iY_i]\n\\end{aligned}\n$$\n\nSince $\\beta$ is a non-random vector, we can remove it from the expectation function.\n\nNow we have a system of linear equations (of the form $Av = b$) with a unique solution if and only if the matrix $E[X_iX_i']$ is invertible. For the inverse of $E[X_iX_i']$ to exist, we require CLRM 4, since $rank(X)=k\\quad a.s.\\Rightarrow rank(E[X_iX_i'])=k$.[^5]\n\n$$\n    \\beta = E[X_iX_i']^{-1}E[X_iY_i]\n$$\n:::\n\n[^5]: For $n$ large, $rank(E[X_iX_i'])=k\\Rightarrow rank(X)=k$. This follows from Law of Large Numbers, since $plim(n^{-1}X'X) = E[X_iX_i']$.\n\nWe cannot compute $\\beta$ because we do not know the joint distribution of $(Y_i,X_i)$ needed to solve for the variance-covariance matrices. However, $\\beta$ is (point) identified because both $Y$ and $X$ are observed in the data and the parameters are \"pinned down\" by a unique set of 'observable' moments in the data.\n\n$\\beta$ is not identified if the above system of linear equations does not have a unique solution. This will occur if two or more of the regressors are perfectly colinear.[^6] $\\beta$ is also not be identified if the resulting expression for $\\beta$ includes 'objects' (moments, distribution/scale parameters) that are not 'observed' in the data. For example, if the error is not mean independent, the above expression will include a bias term that depends on $E[X_i'\\varepsilon_i]$.\n\n[^6]: There are many such failures of (parametric) identification in models that include dummy variables (or fixed effects). Earlier we saw that the intercept is not separately identified from the mean of the error term. Mean independence of the error term, $E[\\varepsilon_i]=0$, is required for us to separately 'identify' $\\beta_1$.\n\nIn this instance, the identification of $\\beta$ is scale dependent. That is, if we multiply $Y_i$ by a scalar, $\\beta$ is multiplied by the same scalar. For example, in cases where a researcher is modelling standardized test-scores.\n\n## Interpretation\n\nIn this linear regression model each slope coefficient has a partial derivative interpretation,\n\n$$\n\\beta_j = \\frac{\\partial E[Y_i|X_i]}{\\partial X_{ij}}\n$$ or, as a vector, $$\n\\beta = \\frac{\\partial E[Y_i|X_i]}{\\partial X_{i}} = \\begin{bmatrix}\\frac{\\partial E[Y_i|X_i]}{\\partial X_{i1}}\\\\ \\vdots \\\\ \\frac{\\partial E[Y_i|X_i]}{\\partial X_{ik}}\\end{bmatrix} = \\begin{bmatrix}\\beta_1\\\\ \\vdots \\\\ \\beta_k\\end{bmatrix}\n$$\n\nNote, the derivative is expressed in terms of changes in the *expected* value of $Y_i$ (conditional on $X_i$), not $Y_i$ itself. This is because $Y_i$ is a random variable, but under CLRM 1 & 2\n\n$$\nE[Y_i|X_i] = X_i'\\beta\n$$\n\nFor a given value of $X_i$, the above expression is non-random.\n\nAs $\\beta_j$ is a partial derivative, its interpretation is one that \"holds fixed\" the value of other regressors (i.e. *ceteris paribus*). Because of this, many researchers apply the experimental language of control variables when interpretting regression coefficients. However, this is dependent on the *assumed* linearity of the CEF.\n\n## Ordinary Least Squares\n\nOLS is *an* estimator for $\\beta$. As will become evident in Lecture 3, it is not the only estimator for $\\beta$.\n\nThe OLS estimator is the solution to,\n\n$$ \n\\min_b\\;\\sum_{i=1}^n(Y_i-X_i'b)^2 \n$$\n\nUsing vector notation, we can rewrite this as\n\n$$ \n\\begin{aligned} \n&\\min_b\\;(Y-Xb)'(Y-Xb)\\\\ \n=&\\min_b\\;Y'Y-Y'Xb-b'X'Y+b'X'Xb \\\\\n=&\\min_b\\;Y'Y-2b'X'Y+b'X'Xb \n\\end{aligned} \n$$ From line 2 to 3 we use the fact that $Y'Xb$ is a scalar and therefore symmetric: $Y'Xb=b'X'Y$.[^7]\n\n[^7]: When working with vectors and matrices it is important to keep track of their size. You can only multiply two matrices/vectors if their column and row dimensions match. For example, if $A$ and $B$ are both $n\\times k$ matrices ($n\\neq k$), then $AB$ is not defined since $A$ has $k$ columns and $B$ $n$ rows. For the same reason $BA$ is also not defined. However, you can pre-multiply $B$ with $A'$ as $A'$ is a $k\\times n$ matrix: $A'B$ is therefore a $(k\\times n)\\cdot (n\\times k)=k\\times k$ matrix. Similarly, $B'A$ is defined, but is a $n\\times n$ matrix.\n\n    Order matters when working with matrices and vectors. Pre-multiplication and post-multiplication are not the same thing.\n\n    Keep track of the size of each term to ensure they correspond to one another. In this instance, each term should be a scalar. For example, $-2b'X'Y$ is the multiplication of a scalar ($-2$: size $1\\times 1$), row vector ($b'$: size $1\\times k$), matrix ($X'$: size $k\\times n$), and column vector ($Y$: size $n\\times 1$). Thus we have a $(1\\times 1)\\cdot (1\\times k)\\cdot (k\\times n)\\cdot (n\\times 1)=1\\times 1$.\n\nDifferentiating the above expression w.r.t. the vector $b$ and setting the first-order conditions to $0$, we find that the following condition must hold for $\\hat{\\beta}$, the solution.\n\n$$ \n  \\begin{aligned} \n  &0=-2X'Y+2X'X\\hat{\\beta} \n  \\\\ \\Rightarrow& X'X\\hat{\\beta} = X'Y \n  \\end{aligned} \n$$\n\n------------------------------------------------------------------------\n\n**How did we get this result?** Deriving the first order conditions requires knowledge of how to solve for the derivative of a scalar respect to a column vector (in this case $b\\in R^k$). The extra material on Linear Algebra has some notes on vector differentiation.\n\nWe can ignore the first term $Y'Y$ as it does not depend on $b$. The second term is $-2b'X'Y$. Here we can use the rule that, $$\n  \\frac{\\partial z'a}{\\partial z} = \\frac{\\partial a'z}{\\partial z} = a\n$$ In this instance, $a = X'Y \\in R^k$. Thus, $$\n  \\frac{\\partial -2b'X'Y}{\\partial b} = -2\\frac{\\partial b'X'Y}{\\partial b} = -2X'Y\n$$ The third term is $b'X'Xb$. This is what is commonly referred to as a quadratic form: $z'Az$. We know that the derivative of this form is, $$\n  \\frac{\\partial z'Az}{\\partial z} = Az + A'z\n$$ and if $A$ is symmetric, the result simplies to $2Az$. In this instance, $A = X'X$ is symmetric and the derivative is given by, $$\n  \\frac{\\partial b'X'Xb}{\\partial b} = 2X'X\n$$\n\n------------------------------------------------------------------------\n\nIn order to solve for $\\hat{\\beta}$ we need to move the $X'X$ term to the right-hand side. If these were scalars we would simply divide both sides by the same constant. However, as $X'X$ is a matrix, division is not possible. Instead, we need to pre-multiply both sides by the inverse of $X'X$: $(X'X)^{-1}$. Here's the issue: the inverse of a matrix need not exist.\n\nGiven a *square* $k\\times k$ matrix $A$, its inverse exists *if and only if* $A$ is non-singular. For $A$ to be non-singular its rank must have full rank: $r(A)=k$, the number of rows/columns. This means that all $k$ columns/rows must be linearly independent. (See Material on Linear Algebra for a more detailed discussion of all these terms.)\n\nIn our application, $A=X'X$ and\n\n$$ r(X'X) = r(X) = colrank(X)\\leq k $$\n\nTo insure that the inverse of $X'X$ exists, $X$ must have full column rank: all column vectors must be *linearly independent*. In practice, this means that no regressor can be a *perfect* linear combination of others. However, we have this from\n\n**CLRM 4:** $rank(X)=k$\n\nYou may know this assumption by another name: the absence of perfect colinearity between regressors.\n\n> The rank condition is the reason we exclude a base category when working with categorical variables.\n>\n> Recall, most linear regression models are specified with a constant. Thus, the first column of $X$ is\n>\n> $$ X_1 = \\begin{bmatrix}1 \\\\ 1 \\\\ \\vdots \\\\ 1\\end{bmatrix} $$ a $n\\times 1$ vector vector of $1$'s, denoted here as $\\ell$. Suppose you have a categorical - for example, gender in an individual level dataset - that splits the same in two. The categories are assumed to be exhaustive and mutually exclusive. If you create two dummy variables, one for each category,\n>\n> $$ X_2 = \\begin{bmatrix}1 \\\\ \\vdots \\\\1\\\\0\\\\ \\vdots \\\\ 0\\end{bmatrix}\\qquad\\text{and}\\qquad X_3 = \\begin{bmatrix}0 \\\\ \\vdots \\\\0\\\\1\\\\ \\vdots \\\\ 1\\end{bmatrix} $$\n>\n> it is evident that $X_2+X_3 = \\ell$. (Here I have depicted the sample as sorted along these two categories.) If $X=[X_1\\;X_2\\;X_3]$, then it is rank-deficient: $r(X) = 2<3$, since $X_3=X_1-X_2$. Thus, we can only include two of these three regressors. We can even exclude the constant and have $X=[X_2\\;X_3]$.\n\nIf $X$ is full rank, then $(X'X)^{-1}$ exists and,\n\n$$ \n\\hat{\\beta} = (X'X)^{-1}X'Y \n$$\n\nThis relatively simple expression is the solution to least squares minimization problem. Just think, it would take less than three lines of code to programme this. That is the power of knowing a little linear algebra.\n\nWe can write the same expression in terms of summations over unit-level observations,\n\n$$ \n\\hat{\\beta} = \\bigg(\\sum_{i=1}^nX_iX_i'\\bigg)^{-1}\\sum_{i=1}^nX_iY_i \n$$\n\nNote, the change in position of the transpose: $X_i$ is a column vector $\\Rightarrow$ $X_i'X_i$ is a scalar while $X_iX_i'$ is a $k\\times k$ matrix. To match the first expression, the term inside the parenthesis must be a $k\\times k$ matrix. Similarly, $X'Y$ is a $k\\times 1$ vector, as is $X_iY_i$.\n\n### Univariate case\n\nUndergraduate textbooks all teach a very similar expression for the OLS estimator of a uni-variate regression model (with a constant); typically, something like,[^8]\n\n[^8]: Once you are familiar with vector notation, it is relatively easy to tell whether a model is uni- or multi-variate. This is because the notation $\\beta_2 X_{i2}$ is not consistent with $X_{2i}$ being a vector (row or column).\n\n    If $X_{i2}$ is a $k\\times 1$ vector then so is $\\beta_2$. Thus, $\\beta_2 X_{i2}$ is $(k\\times 1)\\cdot (k\\times1)$, which is not defined.\n\n    If $X_{i2}$ is a row vector (as in Wooldridge, 2011), $\\beta_2 X_{i2}$ will then be $(k\\times 1)\\cdot (1\\times k)$, a $k\\times k$ matrix. This cannot be correct since the model is defined at the unit level.\n\n    Thus, if you see a model written with the parameter in front of the regressor, you know that this must be a single regressor. This is subtle, yet imporant, distinction that researchers often use to convey the structure of their model. Whenever $X_{i2}$ is a vector, researchers will *almost always* use the notation $X_{i2}'\\beta$ or $X_{i2}\\beta$, depending on whether $X_{i2}$ is assumed to be a column or row vector.\n\n$$\nY_i = \\beta_1+\\beta_2X_{i2}+\\varepsilon_i\n$$ We know that the OLS estimators are give by,\n\n$$\n\\begin{aligned}\n\\tilde{\\beta}_2 =& \\frac{\\sum(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2)}{\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2} \\\\\n\\text{and}\\qquad \\tilde{\\beta}_1 =& \\bar{Y}-\\tilde{\\beta_2}\\bar{X}_2\n\\end{aligned}\n$$\n\nI am deliberately using the notation $\\tilde{\\beta}$ to distinguish these two estimators from the expression below. Let us see if we can replicate this result, using vector notation. The the model is,\n\n$$\n\\begin{aligned}\nY =& X\\beta+\\varepsilon \\\\\n=& \\begin{bmatrix}1&X_{12} \\\\ 1 & X_{22} \\\\ \\vdots & \\vdots \\\\ 1 & X_{n2}\\end{bmatrix}\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\end{bmatrix} + \\varepsilon \\\\\n=& \\begin{bmatrix}\\ell &X_{2} \\end{bmatrix}\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\end{bmatrix} + \\varepsilon\n\\end{aligned}\n$$\n\nTherefore,\n\n$$\n\\begin{aligned} \n\\hat{\\beta} = \\begin{bmatrix}\\hat{\\beta}_1 \\\\ \\hat{\\beta}_2\\end{bmatrix}=&(X'X)^{-1}X'Y \\\\\n=&\\bigg(\\begin{bmatrix}\\ell' \\\\ X_{2}' \\end{bmatrix}\\begin{bmatrix}\\ell &X_{2}\\end{bmatrix}\\bigg)^{-1}\\begin{bmatrix}\\ell'  \\\\ X_{2}' \\end{bmatrix}Y \\\\\n=&\\begin{bmatrix}\\ell'\\ell & \\ell'X_2 \\\\ X_{2}'\\ell & X_2'X_2 \\end{bmatrix}^{-1}\\begin{bmatrix}\\ell'Y  \\\\ X_{2}'Y \\end{bmatrix}\n\\end{aligned} \n$$\n\nI went through this rather quickly, using a number of linear algebra rules that you may not be familiar with. Do not worry, the point of the exercise is not become a linear algebra master, but instead to focus on the element of each of each matrix/vector. Each element is a scalar (size $1\\times 1$).\n\nIf we right them each down as sums you they might be a little more familiar. First consider the $2\\times 2$ matrix:\n\n-   element \\[1,1\\]: $\\ell'\\ell = \\sum_{i=1}^n 1 = n$\n\n-   element \\[1,2\\]: $\\ell'X_2 = \\sum_{i=1}^nX_{i2} = n\\bar{X}_2$\n\n-   element \\[2,1\\]: $X_2'\\ell = \\sum_{i=1}^nX_{i2} = n\\bar{X}_2$ (as above, since scalars are symmetric)\n\n-   element \\[2,2\\]: $X_2'X_2=\\sum_{i=1}^nX_{i2}^2$\n\nNext, consider the final $2\\times 1$ vector,\n\n-   element \\[1,1\\]: $\\ell'Y = \\sum_{i=1}^n Y_i = n\\bar{Y}$\n\n-   element \\[2,1\\]: $X_2'Y = \\sum_{i=1}^nY_iX_{i2}$\n\nOur OLS estimator is therefore,\n\n$$\n\\hat{\\beta} = \\begin{bmatrix} n & n\\bar{X}_2 \\\\ n\\bar{X}_2 & \\sum_{i=1}^nX_{i2}^2 \\end{bmatrix}^{-1}\\begin{bmatrix}n\\bar{Y}  \\\\ \\sum_{i=1}^nY_iX_{i2} \\end{bmatrix}\n$$\n\nWe now need to solve for the inverse of the $2\\times 2$ matrix. You can easily find notes on how to do this online. Here, I will just provide the solution.\n\n$$\n\\hat{\\beta} = \\frac{1}{n\\sum_{i=1}^nX_{i2}^2-n^2\\bar{X}_2^2}\\begin{bmatrix} \\sum_{i=1}^nX_{i2}^2 & -n\\bar{X}_2 \\\\ -n\\bar{X}_2 &  n\\end{bmatrix}\\begin{bmatrix}n\\bar{Y}  \\\\ \\sum_{i=1}^nY_iX_{i2} \\end{bmatrix}\n$$\n\nRemember, this is still a $2\\times 1$ vector. We can now solve for the final solution:\n\n$$\n\\begin{aligned}\n\\hat{\\beta} =& \\frac{1}{n\\sum_{i=1}^nX_{i2}^2-n^2\\bar{X}_2^2}\\begin{bmatrix} n\\bar{Y}\\sum_{i=1}^nX_{i2}^2 -n\\bar{X}_2\\sum_{i=1}^nY_iX_{i2} \\\\ n\\sum_{i=1}^nY_iX_{i2}-n^2\\bar{X}_2\\bar{Y}\\end{bmatrix} \\\\\n=& \\frac{1}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2}\\begin{bmatrix} n\\bar{Y}\\sum_{i=1}^nX_{i2}^2 + n^2\\bar{Y}\\bar{X}^2 - n^2\\bar{Y}\\bar{X}^2 -n\\bar{X}_2\\sum_{i=1}^nY_iX_{i2} \\\\ n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2) \\end{bmatrix} \\\\\n=& \\frac{1}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2}\\begin{bmatrix} n\\bar{Y}\\sum_{i=1}^n(X_{i2}-\\bar{X})^2 -n\\bar{X}_2\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2) \\\\ n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2) \\end{bmatrix} \\\\\n=& \\begin{bmatrix} \\bar{Y}  -\\frac{n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2)}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2}\\bar{X}_2 \\\\ \\frac{n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2)}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2} \\end{bmatrix} \\\\\n=& \\begin{bmatrix} \\bar{Y}  -\\tilde{\\beta}_2\\bar{X}_2 \\\\ \\tilde{\\beta}_2 \\end{bmatrix} \\\\\n=& \\begin{bmatrix}\\tilde{\\beta}_1 \\\\ \\tilde{\\beta}_2 \\end{bmatrix}\n\\end{aligned}\n$$\n\nThe math is a little involved, but it shows you these solutions are are the same. Unfortunately, the working gets even more arduous in a multivariate context. However, there are useful tools to help us with that we will discuss next.\n\n### Geometry of OLS\n\nIn the last section we saw how the OLS estimator can, more generally, be described as a linear transformation of the $Y$ vector.\n\n$$\n\\hat{\\beta} = (X'X)^{-1}X'Y\n$$\n\nWe also saw that in order for there to be a (unique) solution to the least squared problem, the $X$ matrix must be full rank. This rules out any perfect colinearity between columns (i.e. regressors) in the $X$ matrix, including the constant.\n\nGiven the vector of OLS coefficients, we can also estimate the residual,\n\n$$\n\\begin{aligned}\n\\hat{\\varepsilon} =& Y - X\\hat{\\beta} \\\\\n=&Y-X(X'X)^{-1}X'Y \\\\\n=&(I_n-X(X'X)^{-1}X)Y\n\\end{aligned}\n$$\n\nby plugging the definition of $\\hat{\\beta}$. Thus, the OLS estimator separates the vector $Y$ into two components:\n\n$$\n\\begin{aligned}\n Y =& X\\hat{\\beta} + \\hat{\\varepsilon} \\\\\n=&\\underbrace{X(X'X)^{-1}X'}_{P_X}Y + (\\underbrace{I_n-X(X'X)^{-1}X}_{I_n-P_X = M_X})Y \\\\\n=&P_XY + M_XY\n\\end{aligned}\n$$\n\nThe matrix $P_X = X(X'X)^{-1}X'$ is a $n\\times n$ *projection* matrix. It is a linear transformation that projects any vector into the span of $X$: $S(X)\\subset\\mathbb{R}^n$. (See for more information on these terms.) $S(X)$ is the vector space spanned by the columns of $X$. The dimensions of this vector space depends on the rank of $P_X$,\n\n$$\ndim(S(X)) = r(P_X) = r(X) = k\n$$\n\nThe matrix $M_X = I_n-X(X'X)^{-1}X'$ is also a $n\\times n$ projection matrix. It projects any vector into $X$'s *orthogonal* span: $S^{\\perp}(X)$. Any vector $z\\in S^{\\perp}(X)$ is orthogonal to $X$. This includes the estimated residual, which is by definition orthogonal to the predicted values and, indeed, any column of $X$ (i.e. any regressor). The dimension of this orthogonal vector space depends on the rank of $M_X$,\n\n$$\ndim(S^{\\perp}(X)) = r(M_X) = r(I_n)-r(X) = n-k\n$$\n\nThe orthogonality of these two projections can be easily shown, since projection matrices are idempotent ($P_XP_X = P_X$) and symmetric ($P_X' = P_X$). Consider the inner product of these two projections,\n\n$$\nP_X'M_X = P_X(I_n-P_X) = P_X-P_XP_X = P_X-P_X = 0\n$$\n\nThe least squares estimator is a projection of Y into two vector spaces: one the span of the columns of $X$ and the other a space orthogonal to $X$.\n\nWhy is this useful? Well, it helps us understand the \"mechanics\" (technically geometry) of OLS. When working with linear regression models, we typically assume either strict exogeneity - $E[\\varepsilon|X]=0$ - or uncorrelatedness - $E[X'\\varepsilon]=0$ - where the former implies the latter (but not the other way around).\n\nWhen we use OLS, we estimate the vector $\\hat{\\beta}$ such that,\n\n$$\nX'(Y-X\\hat{\\beta})=X'\\hat{\\varepsilon}=0 \\quad always\n$$\n\nThis is true, *not just in expectation*, but by definition. The relationship is \"mechanical\": the regressors and estimated residual are perfectly uncorrelated. This can be easily shown:\n\n$$\n\\begin{aligned}\nX'\\hat{\\varepsilon} =& X'M_XY \\\\\n=& X'(I_n-P_X)Y \\\\\n=&X'I_nY-X'X(X'X)^{-1}X'Y \\\\\n=&X'Y-X'Y \\\\\n=&0\n\\end{aligned}\n$$\n\nYou are essentially imposing the assumption of uncorrelatedness between the explained and unexplained components of Y on the data. This means that if the assumption is wrong, so is the projection.\n\n### Partitioned regression\n\nThe tools of linear algebra can help us better understand partitioned regression. Indeed, I would go as far to to say that it is quite difficult to understand partitioned regression without an understanding of projection matrices. Moreover, we need to understand partitioned regression to really understand multivariate regression. The partitioned regression result is referred to as Frisch-Waugh-Lovell Theorem (FWL).\n\n::: {#thm-fwl title=\"Frisch-Waugh-Lovell (FWL)\"}\nFWL says that if you have two sets of regressors, $[X_1,X_2]$, then $\\hat{\\beta}_1$, the OLS estimator for $\\beta_1$, from the regression,\n\n$$\n        Y = X_1\\beta_1 + X_2\\beta_2 + \\varepsilon\n$$\n\nis also given by the regression,\n\n$$\n        M_2Y = M_2X_1\\beta_1 + \\xi\n$$\n:::\n\nWe can demonstrate the FWL theorem result using projection matrices. Two simplify matters, we will divide the set of regressors into two groups: $X_1$ a single regressor and $X_2$ a $n\\times (k-1)$ matrix. We can rewrite the linear model as,\n\n$$\nY = X\\beta+ \\varepsilon =\\beta_1X_1+X_2\\beta_2+\\varepsilon\n$$\n\nLet us begin by applying our existing knowledge. From above, we know that the residual from the regression of $X_1$ on $X_2$ is,\n\n$$\n\\hat{\\upsilon} = M_2X_1\n$$\n\nwhere $M_2 = I_n-X_2(X_2'X_2)^{-1}X_2'$. It turns out, it does not matter if we residualize $Y$ too. **Can you see why?** Thus, the model we estimate in the second step, is\n\n$$\nY = \\gamma_1\\underbrace{M_2X_1}_{\\hat{\\upsilon}}+\\xi\n$$\n\nWe know that $\\hat{\\gamma}_1 = (\\hat{\\upsilon}'\\hat{\\upsilon})^{-1}\\hat{\\upsilon}'Y$. Replacing the value of the residual, we get,\n\n$$\n\\begin{aligned}\n\\hat{\\gamma}_1 =& (\\hat{\\upsilon}'\\hat{\\upsilon})^{-1}\\hat{\\upsilon}'Y \\\\\n=&(X_1'M_2M_2X_1)^{-1}X_1'M_2Y \\\\\n=&(X_1'M_2X_1)^{-1}X_1'M_2Y\n\\end{aligned}\n$$ Here, we use both the symmetric and idempotent qualities of $M_2$. Next we want to show that $\\hat{\\beta}_1$ is given by the same value. This part is more complicated. Let's start with by reminding ourselves of the following:\n\n$$\n\\begin{aligned}\nX'X\\hat{\\beta} &= X'Y \\\\\n\\begin{bmatrix}X_1 & X_2\\end{bmatrix}'\\begin{bmatrix}X_1 & X_2\\end{bmatrix}\\begin{bmatrix}\\hat{\\beta}_1 \\\\ \\hat{\\beta}_2\\end{bmatrix} &= \\begin{bmatrix}X_1 & X_2\\end{bmatrix}'Y \\\\\n\\begin{bmatrix}X_1'X_1 & X_1'X_2 \\\\ X_2'X_1 & X_2'X_2 \\end{bmatrix}\\begin{bmatrix}\\hat{\\beta}_1 \\\\ \\hat{\\beta}_2\\end{bmatrix} &= \\begin{bmatrix}X_1'Y \\\\ X_2'Y\\end{bmatrix}\n\\end{aligned}\n$$\n\nWe could solve for $\\hat{\\beta}_1$ by solving for the inverse of $X'X$; however, this will take a long time. An easier approach is to simply verify that $\\hat{\\beta}_1=(X_1'M_2X_1)^{-1}X_1'M_2Y$. Recall, $\\hat{\\beta}$ splits $Y$ into two components:\n\n$$\nY = \\hat{\\beta}_1X_1+X_2\\hat{\\beta}_2 + \\hat{\\varepsilon}\n$$\n\nIf we plug this definition of $Y$ into the above expression we get,\n\n$$\n\\begin{aligned}\n&(X_1'M_2X_1)^{-1}X_1'M_2(\\hat{\\beta}_1X_1+X_2\\hat{\\beta}_2 + \\hat{\\varepsilon}) \\\\\n=&\\hat{\\beta}_1\\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2X_1}_{=I_n} \\\\\n&+\\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2X_2\\hat{\\beta_2}}_{=0} \\\\\n&+\\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2\\hat{\\varepsilon}}_{=0} \\\\\n=&\\hat{\\beta}_1\n\\end{aligned}\n$$\n\nIn line 2, I use the fact that $\\hat{\\beta}_1$ is a scalar and can be moved to the front (since the order of multiplication does not matter with a scalar). In line 3, I use the fact that $M_2X_2=0$ by definition. Line 4 uses the fact that $M_2\\hat{\\varepsilon}=\\hat{\\varepsilon}$ which means that $X_1'M_2\\hat{\\varepsilon}=X_1'\\hat{\\varepsilon}=0$.\n\nThe OLS estimator solves for $\\beta_1$ using the variance in $X_1$ that is orthogonal to $X_2$. This is the manner in which we \"hold $X_2$ constant\": the variation in $M_2X_1$ is orthogonal to $X_2$. Changes in $M_2X_1$ are *uncorrelated* with changes in $X_2$; *as if* the variation in $M_2X_1$ arose independently of $X_2$. However, uncorrelatedness does NOT imply independence.\n\n## References\n","srcMarkdownNoYaml":"\n\n## Overview\n\nIn this handout we will revisit the Classical Linear Regression Model (CLRM) [see @wooldridge2010, chap. 1-2]. The goal of this week's lecture is to:\n\n1.  understand the model specification;\n\n2.  it's underlying assumptions;\n\n3.  and the appropriate interpretation;\n\n4.  the OLS estimator, using linear algebra;\n\n5.  the geometry of OLS and partitioned regression result.\n\n## Model Specification\n\nThe linear population regression model is given by,\n\n$$ \n\\begin{aligned}\n  Y_i =& X_i'\\beta+\\varepsilon_i \\\\\n  =& \\beta_1\\mathbf{1}+\\beta_2X_{i2}+\\beta_3X_{i3}+...+\\beta_kX_{ik}+\\varepsilon_i\n\\end{aligned}\n$$\n\nfor $i = 1,2,...,n$. Where,\n\n-   $i$: unit of observation; e.g. individual, firm, union, political party, etc.\n\n-   $Y_i \\in \\mathbb{R}$: scalar random variable.\n\n-   $X_i \\in \\mathbb{R}^k$: $k$-dimensional (column[^1]) vector of regressors, with $k<n$.[^2]\n\n-   $\\beta$: $k$-dimensional, non-random vector of unknown population parameters.\n\n-   $\\varepsilon_i$: *unobserved*, random error term.[^3]\n\n[^1]: My notation assumes that $X_i$ is a column vector, which makes $X_i'\\beta$ a scalar. Wooldridge (2010) uses the notation $X_i\\beta$, implying that $X_i$ is a row vector. This is a matter of preference.\n\n[^2]: You might also refer to the vector of regressors or explanatory variables. The terms covariates or control variables are more common in Microeconometrics literature, where regressors are typically included for identification of causal effects. Some texts will use the term independent variables, but this name implies a specific relationship between $Y$ and $X$ that need not hold. Note, we will asssume in this term that $n>k$; i.e. this is \"small\" data.\n\n[^3]: This is **NOT** the residual.\n\nThe linear population regression equation is **linear in parameters**. This is an important assumption that does NOT restrict the model from being non-linear in regressors. For example, the equation\n\n$$Y_i = \\beta_1 + \\beta_2X_{i2} + \\beta_3X_{i2}^{\\color{red}{2}} + \\varepsilon_i$$ non-linear in $X_{i2}$, but still linear in parameters. In contrast, the equation\n\n$$Y_i = \\beta_1 + \\beta_2X_{i2} + ({\\color{red}{\\beta_2\\beta_3}})X_{i3} + \\varepsilon_i$$ is non-linear in parameters.\n\n### Intercept\n\nThe constant (intercept) in the equation serves an important purpose. While there is no *a priori* reason for the model to have a constant term, it does ensure that the error term is mean zero.\n\n::: proof\nSuppose $E[\\varepsilon_i] = \\gamma$.\n\nWe can then define a new error term, $\\upsilon_i = \\varepsilon_i - \\gamma$, such $E[\\upsilon_i] = 0$. The population regression model can be rewritten as, $$ \\begin{aligned} Y_i =& X_ i'\\beta + v_i + \\gamma  \\\\\n=& \\underbrace{(\\beta_1+\\gamma)}_{\\tilde{\\beta}_1}\\mathbf{1} + \\beta_2X_{i2} + \\beta_3X_{i3} + ... + \\beta_kX_{ik} + v_i\n\\end{aligned}\n$$ The model has a new intercept $\\tilde{\\beta}_1=\\beta_1 + \\gamma$, but the other parameters remain unchanged.\n:::\n\n### Matrix notation\n\nFor a sample of $n$ observations, we can stack the unit-level linear regression equation into a vector,\n\n$$\nY =\\underbrace{\\begin{bmatrix}Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}}_{n\\times 1} = \\underbrace{\\begin{bmatrix}X_1'\\beta \\\\ X_2'\\beta \\\\ \\vdots \\\\ X_n'\\beta\\end{bmatrix}}_{n\\times 1}  + \\underbrace{\\begin{bmatrix}\\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n\\end{bmatrix}}_{n\\times 1}  \n= \\underbrace{\\begin{bmatrix}X_{11} & X_{12} & \\cdots & X_{1k}\\\\ X_{21} & X_{22} && \\\\ \\vdots & & \\ddots & \\\\ X_{n1} & & & X_{nk} \\end{bmatrix}}_{n\\times k} \\underbrace{\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_k\\end{bmatrix}}_{k\\times 1} + \\begin{bmatrix}\\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n\\end{bmatrix} = X\\beta + \\varepsilon\n$$ Notice, in matrix notation, you lose the transpose from $X_i'\\beta$. Apart from the absence of the $i$ subscript, this is a useful way of knowing the dimension of the equation (in my notes). You MUST always write $X\\beta$ and not $\\beta X$. For the scalar case, $X_i'\\beta = \\beta'X_i$, but for the vector case $\\beta X$ is not defined since $\\beta$ is $k\\times 1$ and $X$ is $n\\times k$.\n\n## CLRM Assumptions\n\n**Assumption CLRM 1.** Population regression equation is linear in parameters: $$Y = X\\beta+\\varepsilon$$\n\n**Assumption CLRM 2.** Conditional mean independence of the error term: $$E[\\varepsilon|X]=0$$\n\nAssumption CLRM 2. is stronger than $E[\\varepsilon_i|X_i]$ (mean independence for unit $i$). If all units were independent, then $E[\\varepsilon_i|X_i]$ would imply $E[\\varepsilon|X]=0$. However, since we have not (yet) assumed this, we need this stronger exogeneity assumption. Consider, if $i$ represented units of time ($t$), as in time-series models, independence across $i$ will not hold.\n\nTogether, CLRM 1. and CLRM 2. imply that\n\n$$ E[Y|X] = X\\beta  $$ This means that the Conditional Expectation Function is known and linear in parameters.\n\nConditional mean independence implies - by the Law of Iterated Expectations - mean independence of the error term,\n\n$$E[\\varepsilon|X]=0 \\Rightarrow E\\big[E[\\varepsilon|X]\\big]=E[\\varepsilon]=0$$\n\nand uncorrelatedness,\n\n$$E[\\varepsilon|X]=0 \\Rightarrow E[\\varepsilon X]=0$$ Note, neither of the above statements hold the other way around. Mean independence does not imply conditional mean independence and uncorrelatedness (zero correlation/covariance) does not imply conditional mean independence.\n\nUncorrelatedness rules out linear relationships between the regressors and error term while conditional mean independence rules out non-linear relationships too.\n\nIn general, distributional independence implies mean independence which then implies uncorrelatedness.\n\n::: {#nte-normality}\nIn the case joint-normally distributed random variables, uncorrelatedness implies independence. That is, if $$ \\begin{bmatrix}Y_1 \\\\ Y_2\\end{bmatrix}\\sim N\\bigg(\\begin{bmatrix}\\mu_1 \\\\ \\mu_2\\end{bmatrix},\\begin{bmatrix}\\sigma_1^2 & \\sigma_{12}\\\\ \\sigma_{21} & \\sigma_2^2\\end{bmatrix}\\bigg)$$ Then $\\sigma_{12}=\\sigma_{21}=0 \\iff f_1*f_2 = f_{12}$.\n:::\n\nWe will later show that uncorrelatedness is sufficient for consistency of the Ordinary Least Squares estimater, while conditional mean independence is required for unbiasedness of OLS.\n\n**Assumption CLRM 3.** Homoskedasticity: $Var(\\varepsilon|X) = E[\\varepsilon\\varepsilon'|X] = \\sigma^2I_n$\n\nCLRM 3. states that the variance of the error term is independent of $X$ and constant across units. The diagonal nature of the covariance matrix also implies that the error terms are uncorrelated across units in the data. Note, this does not imply independence of the error terms across units.\n\nModels with heteroskedasticity relax the assumption of constant variance, allowing for a richer variance-covariance matrix that typically depends on $X$.\n\nThis assumption is unlikely to hold in time-series models where units represent repeated observations across time. Such violations are referred to as serial correlation or autocorrelation.\n\nEven in cross-sectional data settings, you can have non-zero correlations across units in the data. A common instance of this is the case of clustering. Clustering can occur when units experience common/correlated 'shocks'; for example, the data contains groups of students from the same classroom who have a the same teacher. This can also be the result of clustered sampling, a common practice in multi-stage survey design.\n\n**Assumption CLRM 4.** Full rank: $rank(X)=k\\quad a.s.$ a.s.[^4]\n\n[^4]: See extra material on Linear Algebra to read more on rank.\n\nSince $X$ is a random variable we should add to the assumption: $rank(X) = k$ *almost surely* (abbreviated a.s.). This means that the set of events in which $X$ is not full rank occur with probability 0. The reason for this addition is that such a set of events (in the sample space) may not be empty.\n\nCLRM 4. is some time referred to as the absence of perfect (or exact) collinearity. Do not confuse this with multicollinearity. Multicollinearity occurs when regressors are highly (linearly) correlated with one another, yielding imprecise estimates.\n\n**Assumption CLRM 5.** Normality of the error term: $\\varepsilon|X \\sim N(0,\\sigma^2 I_n)$\n\n**Assumption CLRM 6.** Observations $\\{(Y_i,X_i): i=1,...,n\\}$ are independently and identically distributed (iid).\n\nCLRM 5 & 6 are not part of the Classical assumptions, but do simplify the problem of inference. Note, CLRM 5 implies independence across error terms, not implied by CLRM 3.\n\n### Non-random $X$\n\nThere is an alternative version of the CLRM in which $X$ is a non-random, matrix of regressors/predictors. With $X$ fixed, the error term is the only only random variable in the model. CLRM assumptions 1 and 4 remain the same, while CLRM 2, 3, 5, and 6 become:\n\n**Assumption CLRM 2^a^.** Mean independence of the error term: $$E[\\varepsilon]=0$$\n\n**Assumption CLRM 3^a^.** Homoskedasticity: $Var(\\varepsilon) = \\sigma^2I_n$\n\n**Assumption CLRM 5^a^.** Normality of the error term: $\\varepsilon \\sim N(0,\\sigma^2 I_n)$\n\n**Assumption CLRM 6^a^.** Observations $\\{\\varepsilon_i: i=1,...,n\\}$ are independently and identically distributed (iid).\n\n### Identification\n\nCLRM 1,2 and 4. are the *identifying* assumptions of the model. These assumptions allow us to write the parameter of interest as a set of 'observable' moments in the data. We can demonstrate this as follows.\n\n::: proof\nStart with CLRM 2.\n\n$$\n    E[\\varepsilon_i|X_i]=0 \n$$\n\nPre-multiply by the vector $X_i$, $$\n        X_iE[\\varepsilon_i|X_i]=0 \n$$ Since the expectation is conditional on $X_i$, we can bring $X_i$ inside the expectation function,\n\n$$\n        E[X_i\\varepsilon_i|X_i]=0 \n    $$ This conditional expectation is a random-function of $X_i$. If we take the expectation of this function w.r.t. $X$, we achieve the aforementioned result that conditional mean independence implies zero covariance, $$\n        E\\left[E[X_i\\varepsilon_i|X_i]\\right]=E[X_i\\varepsilon_i]=0 \n    $$\n\nNow substitute in for $\\varepsilon_i$ using the linear regression model from CLRM 1 and separate the resulting two terms,\n\n$$\n\\begin{aligned}\n    &E[X_i(Y_i-X_i'\\beta)]=0 \\\\\n    \\Rightarrow &E[X_iX_i']\\beta=E[X_iY_i]\n\\end{aligned}\n$$\n\nSince $\\beta$ is a non-random vector, we can remove it from the expectation function.\n\nNow we have a system of linear equations (of the form $Av = b$) with a unique solution if and only if the matrix $E[X_iX_i']$ is invertible. For the inverse of $E[X_iX_i']$ to exist, we require CLRM 4, since $rank(X)=k\\quad a.s.\\Rightarrow rank(E[X_iX_i'])=k$.[^5]\n\n$$\n    \\beta = E[X_iX_i']^{-1}E[X_iY_i]\n$$\n:::\n\n[^5]: For $n$ large, $rank(E[X_iX_i'])=k\\Rightarrow rank(X)=k$. This follows from Law of Large Numbers, since $plim(n^{-1}X'X) = E[X_iX_i']$.\n\nWe cannot compute $\\beta$ because we do not know the joint distribution of $(Y_i,X_i)$ needed to solve for the variance-covariance matrices. However, $\\beta$ is (point) identified because both $Y$ and $X$ are observed in the data and the parameters are \"pinned down\" by a unique set of 'observable' moments in the data.\n\n$\\beta$ is not identified if the above system of linear equations does not have a unique solution. This will occur if two or more of the regressors are perfectly colinear.[^6] $\\beta$ is also not be identified if the resulting expression for $\\beta$ includes 'objects' (moments, distribution/scale parameters) that are not 'observed' in the data. For example, if the error is not mean independent, the above expression will include a bias term that depends on $E[X_i'\\varepsilon_i]$.\n\n[^6]: There are many such failures of (parametric) identification in models that include dummy variables (or fixed effects). Earlier we saw that the intercept is not separately identified from the mean of the error term. Mean independence of the error term, $E[\\varepsilon_i]=0$, is required for us to separately 'identify' $\\beta_1$.\n\nIn this instance, the identification of $\\beta$ is scale dependent. That is, if we multiply $Y_i$ by a scalar, $\\beta$ is multiplied by the same scalar. For example, in cases where a researcher is modelling standardized test-scores.\n\n## Interpretation\n\nIn this linear regression model each slope coefficient has a partial derivative interpretation,\n\n$$\n\\beta_j = \\frac{\\partial E[Y_i|X_i]}{\\partial X_{ij}}\n$$ or, as a vector, $$\n\\beta = \\frac{\\partial E[Y_i|X_i]}{\\partial X_{i}} = \\begin{bmatrix}\\frac{\\partial E[Y_i|X_i]}{\\partial X_{i1}}\\\\ \\vdots \\\\ \\frac{\\partial E[Y_i|X_i]}{\\partial X_{ik}}\\end{bmatrix} = \\begin{bmatrix}\\beta_1\\\\ \\vdots \\\\ \\beta_k\\end{bmatrix}\n$$\n\nNote, the derivative is expressed in terms of changes in the *expected* value of $Y_i$ (conditional on $X_i$), not $Y_i$ itself. This is because $Y_i$ is a random variable, but under CLRM 1 & 2\n\n$$\nE[Y_i|X_i] = X_i'\\beta\n$$\n\nFor a given value of $X_i$, the above expression is non-random.\n\nAs $\\beta_j$ is a partial derivative, its interpretation is one that \"holds fixed\" the value of other regressors (i.e. *ceteris paribus*). Because of this, many researchers apply the experimental language of control variables when interpretting regression coefficients. However, this is dependent on the *assumed* linearity of the CEF.\n\n## Ordinary Least Squares\n\nOLS is *an* estimator for $\\beta$. As will become evident in Lecture 3, it is not the only estimator for $\\beta$.\n\nThe OLS estimator is the solution to,\n\n$$ \n\\min_b\\;\\sum_{i=1}^n(Y_i-X_i'b)^2 \n$$\n\nUsing vector notation, we can rewrite this as\n\n$$ \n\\begin{aligned} \n&\\min_b\\;(Y-Xb)'(Y-Xb)\\\\ \n=&\\min_b\\;Y'Y-Y'Xb-b'X'Y+b'X'Xb \\\\\n=&\\min_b\\;Y'Y-2b'X'Y+b'X'Xb \n\\end{aligned} \n$$ From line 2 to 3 we use the fact that $Y'Xb$ is a scalar and therefore symmetric: $Y'Xb=b'X'Y$.[^7]\n\n[^7]: When working with vectors and matrices it is important to keep track of their size. You can only multiply two matrices/vectors if their column and row dimensions match. For example, if $A$ and $B$ are both $n\\times k$ matrices ($n\\neq k$), then $AB$ is not defined since $A$ has $k$ columns and $B$ $n$ rows. For the same reason $BA$ is also not defined. However, you can pre-multiply $B$ with $A'$ as $A'$ is a $k\\times n$ matrix: $A'B$ is therefore a $(k\\times n)\\cdot (n\\times k)=k\\times k$ matrix. Similarly, $B'A$ is defined, but is a $n\\times n$ matrix.\n\n    Order matters when working with matrices and vectors. Pre-multiplication and post-multiplication are not the same thing.\n\n    Keep track of the size of each term to ensure they correspond to one another. In this instance, each term should be a scalar. For example, $-2b'X'Y$ is the multiplication of a scalar ($-2$: size $1\\times 1$), row vector ($b'$: size $1\\times k$), matrix ($X'$: size $k\\times n$), and column vector ($Y$: size $n\\times 1$). Thus we have a $(1\\times 1)\\cdot (1\\times k)\\cdot (k\\times n)\\cdot (n\\times 1)=1\\times 1$.\n\nDifferentiating the above expression w.r.t. the vector $b$ and setting the first-order conditions to $0$, we find that the following condition must hold for $\\hat{\\beta}$, the solution.\n\n$$ \n  \\begin{aligned} \n  &0=-2X'Y+2X'X\\hat{\\beta} \n  \\\\ \\Rightarrow& X'X\\hat{\\beta} = X'Y \n  \\end{aligned} \n$$\n\n------------------------------------------------------------------------\n\n**How did we get this result?** Deriving the first order conditions requires knowledge of how to solve for the derivative of a scalar respect to a column vector (in this case $b\\in R^k$). The extra material on Linear Algebra has some notes on vector differentiation.\n\nWe can ignore the first term $Y'Y$ as it does not depend on $b$. The second term is $-2b'X'Y$. Here we can use the rule that, $$\n  \\frac{\\partial z'a}{\\partial z} = \\frac{\\partial a'z}{\\partial z} = a\n$$ In this instance, $a = X'Y \\in R^k$. Thus, $$\n  \\frac{\\partial -2b'X'Y}{\\partial b} = -2\\frac{\\partial b'X'Y}{\\partial b} = -2X'Y\n$$ The third term is $b'X'Xb$. This is what is commonly referred to as a quadratic form: $z'Az$. We know that the derivative of this form is, $$\n  \\frac{\\partial z'Az}{\\partial z} = Az + A'z\n$$ and if $A$ is symmetric, the result simplies to $2Az$. In this instance, $A = X'X$ is symmetric and the derivative is given by, $$\n  \\frac{\\partial b'X'Xb}{\\partial b} = 2X'X\n$$\n\n------------------------------------------------------------------------\n\nIn order to solve for $\\hat{\\beta}$ we need to move the $X'X$ term to the right-hand side. If these were scalars we would simply divide both sides by the same constant. However, as $X'X$ is a matrix, division is not possible. Instead, we need to pre-multiply both sides by the inverse of $X'X$: $(X'X)^{-1}$. Here's the issue: the inverse of a matrix need not exist.\n\nGiven a *square* $k\\times k$ matrix $A$, its inverse exists *if and only if* $A$ is non-singular. For $A$ to be non-singular its rank must have full rank: $r(A)=k$, the number of rows/columns. This means that all $k$ columns/rows must be linearly independent. (See Material on Linear Algebra for a more detailed discussion of all these terms.)\n\nIn our application, $A=X'X$ and\n\n$$ r(X'X) = r(X) = colrank(X)\\leq k $$\n\nTo insure that the inverse of $X'X$ exists, $X$ must have full column rank: all column vectors must be *linearly independent*. In practice, this means that no regressor can be a *perfect* linear combination of others. However, we have this from\n\n**CLRM 4:** $rank(X)=k$\n\nYou may know this assumption by another name: the absence of perfect colinearity between regressors.\n\n> The rank condition is the reason we exclude a base category when working with categorical variables.\n>\n> Recall, most linear regression models are specified with a constant. Thus, the first column of $X$ is\n>\n> $$ X_1 = \\begin{bmatrix}1 \\\\ 1 \\\\ \\vdots \\\\ 1\\end{bmatrix} $$ a $n\\times 1$ vector vector of $1$'s, denoted here as $\\ell$. Suppose you have a categorical - for example, gender in an individual level dataset - that splits the same in two. The categories are assumed to be exhaustive and mutually exclusive. If you create two dummy variables, one for each category,\n>\n> $$ X_2 = \\begin{bmatrix}1 \\\\ \\vdots \\\\1\\\\0\\\\ \\vdots \\\\ 0\\end{bmatrix}\\qquad\\text{and}\\qquad X_3 = \\begin{bmatrix}0 \\\\ \\vdots \\\\0\\\\1\\\\ \\vdots \\\\ 1\\end{bmatrix} $$\n>\n> it is evident that $X_2+X_3 = \\ell$. (Here I have depicted the sample as sorted along these two categories.) If $X=[X_1\\;X_2\\;X_3]$, then it is rank-deficient: $r(X) = 2<3$, since $X_3=X_1-X_2$. Thus, we can only include two of these three regressors. We can even exclude the constant and have $X=[X_2\\;X_3]$.\n\nIf $X$ is full rank, then $(X'X)^{-1}$ exists and,\n\n$$ \n\\hat{\\beta} = (X'X)^{-1}X'Y \n$$\n\nThis relatively simple expression is the solution to least squares minimization problem. Just think, it would take less than three lines of code to programme this. That is the power of knowing a little linear algebra.\n\nWe can write the same expression in terms of summations over unit-level observations,\n\n$$ \n\\hat{\\beta} = \\bigg(\\sum_{i=1}^nX_iX_i'\\bigg)^{-1}\\sum_{i=1}^nX_iY_i \n$$\n\nNote, the change in position of the transpose: $X_i$ is a column vector $\\Rightarrow$ $X_i'X_i$ is a scalar while $X_iX_i'$ is a $k\\times k$ matrix. To match the first expression, the term inside the parenthesis must be a $k\\times k$ matrix. Similarly, $X'Y$ is a $k\\times 1$ vector, as is $X_iY_i$.\n\n### Univariate case\n\nUndergraduate textbooks all teach a very similar expression for the OLS estimator of a uni-variate regression model (with a constant); typically, something like,[^8]\n\n[^8]: Once you are familiar with vector notation, it is relatively easy to tell whether a model is uni- or multi-variate. This is because the notation $\\beta_2 X_{i2}$ is not consistent with $X_{2i}$ being a vector (row or column).\n\n    If $X_{i2}$ is a $k\\times 1$ vector then so is $\\beta_2$. Thus, $\\beta_2 X_{i2}$ is $(k\\times 1)\\cdot (k\\times1)$, which is not defined.\n\n    If $X_{i2}$ is a row vector (as in Wooldridge, 2011), $\\beta_2 X_{i2}$ will then be $(k\\times 1)\\cdot (1\\times k)$, a $k\\times k$ matrix. This cannot be correct since the model is defined at the unit level.\n\n    Thus, if you see a model written with the parameter in front of the regressor, you know that this must be a single regressor. This is subtle, yet imporant, distinction that researchers often use to convey the structure of their model. Whenever $X_{i2}$ is a vector, researchers will *almost always* use the notation $X_{i2}'\\beta$ or $X_{i2}\\beta$, depending on whether $X_{i2}$ is assumed to be a column or row vector.\n\n$$\nY_i = \\beta_1+\\beta_2X_{i2}+\\varepsilon_i\n$$ We know that the OLS estimators are give by,\n\n$$\n\\begin{aligned}\n\\tilde{\\beta}_2 =& \\frac{\\sum(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2)}{\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2} \\\\\n\\text{and}\\qquad \\tilde{\\beta}_1 =& \\bar{Y}-\\tilde{\\beta_2}\\bar{X}_2\n\\end{aligned}\n$$\n\nI am deliberately using the notation $\\tilde{\\beta}$ to distinguish these two estimators from the expression below. Let us see if we can replicate this result, using vector notation. The the model is,\n\n$$\n\\begin{aligned}\nY =& X\\beta+\\varepsilon \\\\\n=& \\begin{bmatrix}1&X_{12} \\\\ 1 & X_{22} \\\\ \\vdots & \\vdots \\\\ 1 & X_{n2}\\end{bmatrix}\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\end{bmatrix} + \\varepsilon \\\\\n=& \\begin{bmatrix}\\ell &X_{2} \\end{bmatrix}\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\end{bmatrix} + \\varepsilon\n\\end{aligned}\n$$\n\nTherefore,\n\n$$\n\\begin{aligned} \n\\hat{\\beta} = \\begin{bmatrix}\\hat{\\beta}_1 \\\\ \\hat{\\beta}_2\\end{bmatrix}=&(X'X)^{-1}X'Y \\\\\n=&\\bigg(\\begin{bmatrix}\\ell' \\\\ X_{2}' \\end{bmatrix}\\begin{bmatrix}\\ell &X_{2}\\end{bmatrix}\\bigg)^{-1}\\begin{bmatrix}\\ell'  \\\\ X_{2}' \\end{bmatrix}Y \\\\\n=&\\begin{bmatrix}\\ell'\\ell & \\ell'X_2 \\\\ X_{2}'\\ell & X_2'X_2 \\end{bmatrix}^{-1}\\begin{bmatrix}\\ell'Y  \\\\ X_{2}'Y \\end{bmatrix}\n\\end{aligned} \n$$\n\nI went through this rather quickly, using a number of linear algebra rules that you may not be familiar with. Do not worry, the point of the exercise is not become a linear algebra master, but instead to focus on the element of each of each matrix/vector. Each element is a scalar (size $1\\times 1$).\n\nIf we right them each down as sums you they might be a little more familiar. First consider the $2\\times 2$ matrix:\n\n-   element \\[1,1\\]: $\\ell'\\ell = \\sum_{i=1}^n 1 = n$\n\n-   element \\[1,2\\]: $\\ell'X_2 = \\sum_{i=1}^nX_{i2} = n\\bar{X}_2$\n\n-   element \\[2,1\\]: $X_2'\\ell = \\sum_{i=1}^nX_{i2} = n\\bar{X}_2$ (as above, since scalars are symmetric)\n\n-   element \\[2,2\\]: $X_2'X_2=\\sum_{i=1}^nX_{i2}^2$\n\nNext, consider the final $2\\times 1$ vector,\n\n-   element \\[1,1\\]: $\\ell'Y = \\sum_{i=1}^n Y_i = n\\bar{Y}$\n\n-   element \\[2,1\\]: $X_2'Y = \\sum_{i=1}^nY_iX_{i2}$\n\nOur OLS estimator is therefore,\n\n$$\n\\hat{\\beta} = \\begin{bmatrix} n & n\\bar{X}_2 \\\\ n\\bar{X}_2 & \\sum_{i=1}^nX_{i2}^2 \\end{bmatrix}^{-1}\\begin{bmatrix}n\\bar{Y}  \\\\ \\sum_{i=1}^nY_iX_{i2} \\end{bmatrix}\n$$\n\nWe now need to solve for the inverse of the $2\\times 2$ matrix. You can easily find notes on how to do this online. Here, I will just provide the solution.\n\n$$\n\\hat{\\beta} = \\frac{1}{n\\sum_{i=1}^nX_{i2}^2-n^2\\bar{X}_2^2}\\begin{bmatrix} \\sum_{i=1}^nX_{i2}^2 & -n\\bar{X}_2 \\\\ -n\\bar{X}_2 &  n\\end{bmatrix}\\begin{bmatrix}n\\bar{Y}  \\\\ \\sum_{i=1}^nY_iX_{i2} \\end{bmatrix}\n$$\n\nRemember, this is still a $2\\times 1$ vector. We can now solve for the final solution:\n\n$$\n\\begin{aligned}\n\\hat{\\beta} =& \\frac{1}{n\\sum_{i=1}^nX_{i2}^2-n^2\\bar{X}_2^2}\\begin{bmatrix} n\\bar{Y}\\sum_{i=1}^nX_{i2}^2 -n\\bar{X}_2\\sum_{i=1}^nY_iX_{i2} \\\\ n\\sum_{i=1}^nY_iX_{i2}-n^2\\bar{X}_2\\bar{Y}\\end{bmatrix} \\\\\n=& \\frac{1}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2}\\begin{bmatrix} n\\bar{Y}\\sum_{i=1}^nX_{i2}^2 + n^2\\bar{Y}\\bar{X}^2 - n^2\\bar{Y}\\bar{X}^2 -n\\bar{X}_2\\sum_{i=1}^nY_iX_{i2} \\\\ n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2) \\end{bmatrix} \\\\\n=& \\frac{1}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2}\\begin{bmatrix} n\\bar{Y}\\sum_{i=1}^n(X_{i2}-\\bar{X})^2 -n\\bar{X}_2\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2) \\\\ n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2) \\end{bmatrix} \\\\\n=& \\begin{bmatrix} \\bar{Y}  -\\frac{n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2)}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2}\\bar{X}_2 \\\\ \\frac{n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2)}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2} \\end{bmatrix} \\\\\n=& \\begin{bmatrix} \\bar{Y}  -\\tilde{\\beta}_2\\bar{X}_2 \\\\ \\tilde{\\beta}_2 \\end{bmatrix} \\\\\n=& \\begin{bmatrix}\\tilde{\\beta}_1 \\\\ \\tilde{\\beta}_2 \\end{bmatrix}\n\\end{aligned}\n$$\n\nThe math is a little involved, but it shows you these solutions are are the same. Unfortunately, the working gets even more arduous in a multivariate context. However, there are useful tools to help us with that we will discuss next.\n\n### Geometry of OLS\n\nIn the last section we saw how the OLS estimator can, more generally, be described as a linear transformation of the $Y$ vector.\n\n$$\n\\hat{\\beta} = (X'X)^{-1}X'Y\n$$\n\nWe also saw that in order for there to be a (unique) solution to the least squared problem, the $X$ matrix must be full rank. This rules out any perfect colinearity between columns (i.e. regressors) in the $X$ matrix, including the constant.\n\nGiven the vector of OLS coefficients, we can also estimate the residual,\n\n$$\n\\begin{aligned}\n\\hat{\\varepsilon} =& Y - X\\hat{\\beta} \\\\\n=&Y-X(X'X)^{-1}X'Y \\\\\n=&(I_n-X(X'X)^{-1}X)Y\n\\end{aligned}\n$$\n\nby plugging the definition of $\\hat{\\beta}$. Thus, the OLS estimator separates the vector $Y$ into two components:\n\n$$\n\\begin{aligned}\n Y =& X\\hat{\\beta} + \\hat{\\varepsilon} \\\\\n=&\\underbrace{X(X'X)^{-1}X'}_{P_X}Y + (\\underbrace{I_n-X(X'X)^{-1}X}_{I_n-P_X = M_X})Y \\\\\n=&P_XY + M_XY\n\\end{aligned}\n$$\n\nThe matrix $P_X = X(X'X)^{-1}X'$ is a $n\\times n$ *projection* matrix. It is a linear transformation that projects any vector into the span of $X$: $S(X)\\subset\\mathbb{R}^n$. (See for more information on these terms.) $S(X)$ is the vector space spanned by the columns of $X$. The dimensions of this vector space depends on the rank of $P_X$,\n\n$$\ndim(S(X)) = r(P_X) = r(X) = k\n$$\n\nThe matrix $M_X = I_n-X(X'X)^{-1}X'$ is also a $n\\times n$ projection matrix. It projects any vector into $X$'s *orthogonal* span: $S^{\\perp}(X)$. Any vector $z\\in S^{\\perp}(X)$ is orthogonal to $X$. This includes the estimated residual, which is by definition orthogonal to the predicted values and, indeed, any column of $X$ (i.e. any regressor). The dimension of this orthogonal vector space depends on the rank of $M_X$,\n\n$$\ndim(S^{\\perp}(X)) = r(M_X) = r(I_n)-r(X) = n-k\n$$\n\nThe orthogonality of these two projections can be easily shown, since projection matrices are idempotent ($P_XP_X = P_X$) and symmetric ($P_X' = P_X$). Consider the inner product of these two projections,\n\n$$\nP_X'M_X = P_X(I_n-P_X) = P_X-P_XP_X = P_X-P_X = 0\n$$\n\nThe least squares estimator is a projection of Y into two vector spaces: one the span of the columns of $X$ and the other a space orthogonal to $X$.\n\nWhy is this useful? Well, it helps us understand the \"mechanics\" (technically geometry) of OLS. When working with linear regression models, we typically assume either strict exogeneity - $E[\\varepsilon|X]=0$ - or uncorrelatedness - $E[X'\\varepsilon]=0$ - where the former implies the latter (but not the other way around).\n\nWhen we use OLS, we estimate the vector $\\hat{\\beta}$ such that,\n\n$$\nX'(Y-X\\hat{\\beta})=X'\\hat{\\varepsilon}=0 \\quad always\n$$\n\nThis is true, *not just in expectation*, but by definition. The relationship is \"mechanical\": the regressors and estimated residual are perfectly uncorrelated. This can be easily shown:\n\n$$\n\\begin{aligned}\nX'\\hat{\\varepsilon} =& X'M_XY \\\\\n=& X'(I_n-P_X)Y \\\\\n=&X'I_nY-X'X(X'X)^{-1}X'Y \\\\\n=&X'Y-X'Y \\\\\n=&0\n\\end{aligned}\n$$\n\nYou are essentially imposing the assumption of uncorrelatedness between the explained and unexplained components of Y on the data. This means that if the assumption is wrong, so is the projection.\n\n### Partitioned regression\n\nThe tools of linear algebra can help us better understand partitioned regression. Indeed, I would go as far to to say that it is quite difficult to understand partitioned regression without an understanding of projection matrices. Moreover, we need to understand partitioned regression to really understand multivariate regression. The partitioned regression result is referred to as Frisch-Waugh-Lovell Theorem (FWL).\n\n::: {#thm-fwl title=\"Frisch-Waugh-Lovell (FWL)\"}\nFWL says that if you have two sets of regressors, $[X_1,X_2]$, then $\\hat{\\beta}_1$, the OLS estimator for $\\beta_1$, from the regression,\n\n$$\n        Y = X_1\\beta_1 + X_2\\beta_2 + \\varepsilon\n$$\n\nis also given by the regression,\n\n$$\n        M_2Y = M_2X_1\\beta_1 + \\xi\n$$\n:::\n\nWe can demonstrate the FWL theorem result using projection matrices. Two simplify matters, we will divide the set of regressors into two groups: $X_1$ a single regressor and $X_2$ a $n\\times (k-1)$ matrix. We can rewrite the linear model as,\n\n$$\nY = X\\beta+ \\varepsilon =\\beta_1X_1+X_2\\beta_2+\\varepsilon\n$$\n\nLet us begin by applying our existing knowledge. From above, we know that the residual from the regression of $X_1$ on $X_2$ is,\n\n$$\n\\hat{\\upsilon} = M_2X_1\n$$\n\nwhere $M_2 = I_n-X_2(X_2'X_2)^{-1}X_2'$. It turns out, it does not matter if we residualize $Y$ too. **Can you see why?** Thus, the model we estimate in the second step, is\n\n$$\nY = \\gamma_1\\underbrace{M_2X_1}_{\\hat{\\upsilon}}+\\xi\n$$\n\nWe know that $\\hat{\\gamma}_1 = (\\hat{\\upsilon}'\\hat{\\upsilon})^{-1}\\hat{\\upsilon}'Y$. Replacing the value of the residual, we get,\n\n$$\n\\begin{aligned}\n\\hat{\\gamma}_1 =& (\\hat{\\upsilon}'\\hat{\\upsilon})^{-1}\\hat{\\upsilon}'Y \\\\\n=&(X_1'M_2M_2X_1)^{-1}X_1'M_2Y \\\\\n=&(X_1'M_2X_1)^{-1}X_1'M_2Y\n\\end{aligned}\n$$ Here, we use both the symmetric and idempotent qualities of $M_2$. Next we want to show that $\\hat{\\beta}_1$ is given by the same value. This part is more complicated. Let's start with by reminding ourselves of the following:\n\n$$\n\\begin{aligned}\nX'X\\hat{\\beta} &= X'Y \\\\\n\\begin{bmatrix}X_1 & X_2\\end{bmatrix}'\\begin{bmatrix}X_1 & X_2\\end{bmatrix}\\begin{bmatrix}\\hat{\\beta}_1 \\\\ \\hat{\\beta}_2\\end{bmatrix} &= \\begin{bmatrix}X_1 & X_2\\end{bmatrix}'Y \\\\\n\\begin{bmatrix}X_1'X_1 & X_1'X_2 \\\\ X_2'X_1 & X_2'X_2 \\end{bmatrix}\\begin{bmatrix}\\hat{\\beta}_1 \\\\ \\hat{\\beta}_2\\end{bmatrix} &= \\begin{bmatrix}X_1'Y \\\\ X_2'Y\\end{bmatrix}\n\\end{aligned}\n$$\n\nWe could solve for $\\hat{\\beta}_1$ by solving for the inverse of $X'X$; however, this will take a long time. An easier approach is to simply verify that $\\hat{\\beta}_1=(X_1'M_2X_1)^{-1}X_1'M_2Y$. Recall, $\\hat{\\beta}$ splits $Y$ into two components:\n\n$$\nY = \\hat{\\beta}_1X_1+X_2\\hat{\\beta}_2 + \\hat{\\varepsilon}\n$$\n\nIf we plug this definition of $Y$ into the above expression we get,\n\n$$\n\\begin{aligned}\n&(X_1'M_2X_1)^{-1}X_1'M_2(\\hat{\\beta}_1X_1+X_2\\hat{\\beta}_2 + \\hat{\\varepsilon}) \\\\\n=&\\hat{\\beta}_1\\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2X_1}_{=I_n} \\\\\n&+\\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2X_2\\hat{\\beta_2}}_{=0} \\\\\n&+\\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2\\hat{\\varepsilon}}_{=0} \\\\\n=&\\hat{\\beta}_1\n\\end{aligned}\n$$\n\nIn line 2, I use the fact that $\\hat{\\beta}_1$ is a scalar and can be moved to the front (since the order of multiplication does not matter with a scalar). In line 3, I use the fact that $M_2X_2=0$ by definition. Line 4 uses the fact that $M_2\\hat{\\varepsilon}=\\hat{\\varepsilon}$ which means that $X_1'M_2\\hat{\\varepsilon}=X_1'\\hat{\\varepsilon}=0$.\n\nThe OLS estimator solves for $\\beta_1$ using the variance in $X_1$ that is orthogonal to $X_2$. This is the manner in which we \"hold $X_2$ constant\": the variation in $M_2X_1$ is orthogonal to $X_2$. Changes in $M_2X_1$ are *uncorrelated* with changes in $X_2$; *as if* the variation in $M_2X_1$ arose independently of $X_2$. However, uncorrelatedness does NOT imply independence.\n\n## References\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"number-sections":true,"output-file":"handout-1neil.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.42","theme":{"light":"lightly","dark":"darkly"},"title":"Classical Linear Regression Model & Ordinary Least Squares","resources":["handout-1.pdf"]},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":false,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","toc":true,"number-sections":true,"output-file":"handout-1.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"title":"Classical Linear Regression Model & Ordinary Least Squares"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}