[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EC226",
    "section": "",
    "text": "Welcome. Use the menu to access weekly lecture notes and other materials.\n\n\n\nThe HTML pages include interactive R code you can edit and run in the browser.\nEach week also has a downloadable PDF version."
  },
  {
    "objectID": "index.html#how-to-use-these-notes",
    "href": "index.html#how-to-use-these-notes",
    "title": "EC226",
    "section": "",
    "text": "The HTML pages include interactive R code you can edit and run in the browser.\nEach week also has a downloadable PDF version."
  },
  {
    "objectID": "week01.html",
    "href": "week01.html",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "",
    "text": "Note\n\n\n\nDownload PDF"
  },
  {
    "objectID": "week01.html#readings",
    "href": "week01.html#readings",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "1.1 Readings",
    "text": "1.1 Readings\n\nStock and Watson (2003), Chapters 4‚Äì5\nDougherty (2016), Chapter 2\nWooldridge (2013), Chapter 2"
  },
  {
    "objectID": "week01.html#correlation",
    "href": "week01.html#correlation",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "3.1 Correlation",
    "text": "3.1 Correlation\nIn economics we are interested in the relationship between two or more random variables, for example: - Sales and advertising expenditure - Personal consumption and disposable income - Investment and interest rates - Earnings and schooling A measure of linear association between two random variables \\(x\\) and \\(y\\) is the covariance, which for a sample of \\(n\\) pairs of observations \\((x_1, y_1)\\), \\(\\ldots\\), \\((x_n, y_n)\\) is calculated as\n\\[\n\\operatorname{cov}(x,y)\n=\n\\frac{1}{n-1}\n\\sum_{i=1}^{n}\n(x_i - \\bar{x})(y_i - \\bar{y})\n\\]\nThe covariance is a measure of linear association. It may be approximately zero even when there is a strong non-linear (e.g.¬†quadratic) relationship."
  },
  {
    "objectID": "test-interactive.html",
    "href": "test-interactive.html",
    "title": "Test Interactive Correlation",
    "section": "",
    "text": "Interactive correlation demo\n\n\n\n&lt;label&gt;&lt;strong&gt;œÅ&lt;/strong&gt;\n  &lt;input id=\"rho\" type=\"range\" min=\"-0.95\" max=\"0.95\" step=\"0.01\" value=\"0.70\"&gt;\n  &lt;span id=\"rhoOut\" style=\"font-family: monospace;\"&gt;0.70&lt;/span&gt;\n&lt;/label&gt;\n\n&lt;label&gt;&lt;strong&gt;n&lt;/strong&gt;\n  &lt;input id=\"n\" type=\"range\" min=\"20\" max=\"300\" step=\"10\" value=\"100\"&gt;\n  &lt;span id=\"nOut\" style=\"font-family: monospace;\"&gt;100&lt;/span&gt;\n&lt;/label&gt;\n\n&lt;button id=\"resample\" type=\"button\"&gt;Resample&lt;/button&gt;\n&lt;div&gt;&lt;strong&gt;corr:&lt;/strong&gt; &lt;span id=\"corr\" style=\"font-family: monospace;\"&gt;...&lt;/span&gt;&lt;/div&gt;"
  },
  {
    "objectID": "week01.html#overview",
    "href": "week01.html#overview",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "1.2 Overview",
    "text": "1.2 Overview\nIn this handout we will revisit the Classical Linear Regression Model (CLRM) [see @wooldridge2010, chap. 1-2]. The goal of this week‚Äôs lecture is to: 1. understand the model specification; 2. it‚Äôs underlying assumptions; 3. and the appropriate interpretation; 4. the OLS estimator, using linear algebra; 5. the geometry of OLS and partitioned regression result."
  },
  {
    "objectID": "week01.html#model-specification",
    "href": "week01.html#model-specification",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "1.3 Model Specification",
    "text": "1.3 Model Specification\nThe linear population regression model is given by,\n\\[\n\\begin{aligned}\n  Y_i =& X_i'\\beta+\\varepsilon_i \\\\\n  =& \\beta_1\\mathbf{1}+\\beta_2X_{i2}+\\beta_3X_{i3}+...+\\beta_kX_{ik}+\\varepsilon_i\n\\end{aligned}\n\\]\n\n\nCode\n1+1\n\n\n[1] 2\n\n\nCode\nqlfs18 &lt;- haven::read_dta(\"data/qlfs1314_2.dta\")\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(haven)\n\n# Read the Stata dataset (adjust path if needed)\nqlfs18 &lt;- read_dta(\"data/qlfs1314_2.dta\")\n\n# Show variables so you can verify names during render\nhead(names(qlfs18), 30)\n\n\n [1] \"age\"     \"cameyr\"  \"cry01\"   \"degcls\"  \"edage\"   \"hdpch19\" \"hhtype\" \n [8] \"higho\"   \"hiqual\"  \"hourpay\" \"marsta\"  \"nsecmmj\" \"sc2kmmj\" \"sex\"    \n[15] \"sngdeg\"  \"soc2kmn\" \"ttushr\"  \"uresmc\"  \"nation\"  \"eth01\"   \"relig\"  \n[22] \"hiquala\" \"hiqualv\" \"sctqual\" \"yr\"      \"tenure\"  \"hpay\"    \"lhpay\"  \n[29] \"married\" \"white\"  \n\n\nCode\n# Rename if those vars exist\nif (\"HOURPAY\" %in% names(qlfs18)) {\n  qlfs18 &lt;- qlfs18 |&gt; rename(hourpay = HOURPAY)\n}\nif (\"EDAGE\" %in% names(qlfs18)) {\n  qlfs18 &lt;- qlfs18 |&gt; rename(edage = EDAGE)\n}\nif (\"AGE\" %in% names(qlfs18)) {\n  qlfs18 &lt;- qlfs18 |&gt; rename(age = AGE)\n}\n\n# Only plot if hourpay exists\nif (\"hourpay\" %in% names(qlfs18)) {\n  ggplot(qlfs18, aes(x = hourpay)) +\n    geom_histogram(bins = 30)\n} else {\n  cat(\"hourpay not found ‚Äî check the variable name in the dataset.\\n\")\n}\n\n\nError in list2(na.rm = na.rm, binwidth = binwidth, bins = bins, orientation = orientation, : object 'ffi_list2' not found\n\n\nCode\n# Create log wage if possible\nif (\"hourpay\" %in% names(qlfs18)) {\n  qlfs18 &lt;- qlfs18 |&gt; mutate(lhourpay = log(hourpay))\n  summary(qlfs18$lhourpay)\n}\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n -2.040   2.175   2.526   2.553   2.902   6.880   14658"
  },
  {
    "objectID": "handout-1neil.html",
    "href": "handout-1neil.html",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "",
    "text": "In this handout we will revisit the Classical Linear Regression Model (CLRM) [see @wooldridge2010, chap. 1-2]. The goal of this week‚Äôs lecture is to:\n\nunderstand the model specification;\nit‚Äôs underlying assumptions;\nand the appropriate interpretation;\nthe OLS estimator, using linear algebra;\nthe geometry of OLS and partitioned regression result."
  },
  {
    "objectID": "handout-1neil.html#overview",
    "href": "handout-1neil.html#overview",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "",
    "text": "In this handout we will revisit the Classical Linear Regression Model (CLRM) [see @wooldridge2010, chap. 1-2]. The goal of this week‚Äôs lecture is to:\n\nunderstand the model specification;\nit‚Äôs underlying assumptions;\nand the appropriate interpretation;\nthe OLS estimator, using linear algebra;\nthe geometry of OLS and partitioned regression result."
  },
  {
    "objectID": "handout-1neil.html#model-specification",
    "href": "handout-1neil.html#model-specification",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "2 Model Specification",
    "text": "2 Model Specification\nThe linear population regression model is given by,\n\\[\n\\begin{aligned}\n  Y_i =& X_i'\\beta+\\varepsilon_i \\\\\n  =& \\beta_1\\mathbf{1}+\\beta_2X_{i2}+\\beta_3X_{i3}+...+\\beta_kX_{ik}+\\varepsilon_i\n\\end{aligned}\n\\]\nfor \\(i = 1,2,...,n\\). Where,\n\n\\(i\\): unit of observation; e.g.¬†individual, firm, union, political party, etc.\n\\(Y_i \\in \\mathbb{R}\\): scalar random variable.\n\\(X_i \\in \\mathbb{R}^k\\): \\(k\\)-dimensional (column1) vector of regressors, with \\(k&lt;n\\).2\n\\(\\beta\\): \\(k\\)-dimensional, non-random vector of unknown population parameters.\n\\(\\varepsilon_i\\): unobserved, random error term.3\n\nThe linear population regression equation is linear in parameters. This is an important assumption that does NOT restrict the model from being non-linear in regressors. For example, the equation\n\\[Y_i = \\beta_1 + \\beta_2X_{i2} + \\beta_3X_{i2}^{\\color{red}{2}} + \\varepsilon_i\\] non-linear in \\(X_{i2}\\), but still linear in parameters. In contrast, the equation\n\\[Y_i = \\beta_1 + \\beta_2X_{i2} + ({\\color{red}{\\beta_2\\beta_3}})X_{i3} + \\varepsilon_i\\] is non-linear in parameters.\n\n2.1 Intercept\nThe constant (intercept) in the equation serves an important purpose. While there is no a priori reason for the model to have a constant term, it does ensure that the error term is mean zero.\n\nProof. Suppose \\(E[\\varepsilon_i] = \\gamma\\).\nWe can then define a new error term, \\(\\upsilon_i = \\varepsilon_i - \\gamma\\), such \\(E[\\upsilon_i] = 0\\). The population regression model can be rewritten as, \\[ \\begin{aligned} Y_i =& X_ i'\\beta + v_i + \\gamma  \\\\\n=& \\underbrace{(\\beta_1+\\gamma)}_{\\tilde{\\beta}_1}\\mathbf{1} + \\beta_2X_{i2} + \\beta_3X_{i3} + ... + \\beta_kX_{ik} + v_i\n\\end{aligned}\n\\] The model has a new intercept \\(\\tilde{\\beta}_1=\\beta_1 + \\gamma\\), but the other parameters remain unchanged.\n\n\n\n2.2 Matrix notation\nFor a sample of \\(n\\) observations, we can stack the unit-level linear regression equation into a vector,\n\\[\nY =\\underbrace{\\begin{bmatrix}Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}}_{n\\times 1} = \\underbrace{\\begin{bmatrix}X_1'\\beta \\\\ X_2'\\beta \\\\ \\vdots \\\\ X_n'\\beta\\end{bmatrix}}_{n\\times 1}  + \\underbrace{\\begin{bmatrix}\\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n\\end{bmatrix}}_{n\\times 1}  \n= \\underbrace{\\begin{bmatrix}X_{11} & X_{12} & \\cdots & X_{1k}\\\\ X_{21} & X_{22} && \\\\ \\vdots & & \\ddots & \\\\ X_{n1} & & & X_{nk} \\end{bmatrix}}_{n\\times k} \\underbrace{\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_k\\end{bmatrix}}_{k\\times 1} + \\begin{bmatrix}\\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n\\end{bmatrix} = X\\beta + \\varepsilon\n\\] Notice, in matrix notation, you lose the transpose from \\(X_i'\\beta\\). Apart from the absence of the \\(i\\) subscript, this is a useful way of knowing the dimension of the equation (in my notes). You MUST always write \\(X\\beta\\) and not \\(\\beta X\\). For the scalar case, \\(X_i'\\beta = \\beta'X_i\\), but for the vector case \\(\\beta X\\) is not defined since \\(\\beta\\) is \\(k\\times 1\\) and \\(X\\) is \\(n\\times k\\)."
  },
  {
    "objectID": "handout-1neil.html#clrm-assumptions",
    "href": "handout-1neil.html#clrm-assumptions",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "3 CLRM Assumptions",
    "text": "3 CLRM Assumptions\nAssumption CLRM 1. Population regression equation is linear in parameters: \\[Y = X\\beta+\\varepsilon\\]\nAssumption CLRM 2. Conditional mean independence of the error term: \\[E[\\varepsilon|X]=0\\]\nAssumption CLRM 2. is stronger than \\(E[\\varepsilon_i|X_i]\\) (mean independence for unit \\(i\\)). If all units were independent, then \\(E[\\varepsilon_i|X_i]\\) would imply \\(E[\\varepsilon|X]=0\\). However, since we have not (yet) assumed this, we need this stronger exogeneity assumption. Consider, if \\(i\\) represented units of time (\\(t\\)), as in time-series models, independence across \\(i\\) will not hold.\nTogether, CLRM 1. and CLRM 2. imply that\n\\[ E[Y|X] = X\\beta  \\] This means that the Conditional Expectation Function is known and linear in parameters.\nConditional mean independence implies - by the Law of Iterated Expectations - mean independence of the error term,\n\\[E[\\varepsilon|X]=0 \\Rightarrow E\\big[E[\\varepsilon|X]\\big]=E[\\varepsilon]=0\\]\nand uncorrelatedness,\n\\[E[\\varepsilon|X]=0 \\Rightarrow E[\\varepsilon X]=0\\] Note, neither of the above statements hold the other way around. Mean independence does not imply conditional mean independence and uncorrelatedness (zero correlation/covariance) does not imply conditional mean independence.\nUncorrelatedness rules out linear relationships between the regressors and error term while conditional mean independence rules out non-linear relationships too.\nIn general, distributional independence implies mean independence which then implies uncorrelatedness.\n\nIn the case joint-normally distributed random variables, uncorrelatedness implies independence. That is, if \\[ \\begin{bmatrix}Y_1 \\\\ Y_2\\end{bmatrix}\\sim N\\bigg(\\begin{bmatrix}\\mu_1 \\\\ \\mu_2\\end{bmatrix},\\begin{bmatrix}\\sigma_1^2 & \\sigma_{12}\\\\ \\sigma_{21} & \\sigma_2^2\\end{bmatrix}\\bigg)\\] Then \\(\\sigma_{12}=\\sigma_{21}=0 \\iff f_1*f_2 = f_{12}\\).\n\nWe will later show that uncorrelatedness is sufficient for consistency of the Ordinary Least Squares estimater, while conditional mean independence is required for unbiasedness of OLS.\nAssumption CLRM 3. Homoskedasticity: \\(Var(\\varepsilon|X) = E[\\varepsilon\\varepsilon'|X] = \\sigma^2I_n\\)\nCLRM 3. states that the variance of the error term is independent of \\(X\\) and constant across units. The diagonal nature of the covariance matrix also implies that the error terms are uncorrelated across units in the data. Note, this does not imply independence of the error terms across units.\nModels with heteroskedasticity relax the assumption of constant variance, allowing for a richer variance-covariance matrix that typically depends on \\(X\\).\nThis assumption is unlikely to hold in time-series models where units represent repeated observations across time. Such violations are referred to as serial correlation or autocorrelation.\nEven in cross-sectional data settings, you can have non-zero correlations across units in the data. A common instance of this is the case of clustering. Clustering can occur when units experience common/correlated ‚Äòshocks‚Äô; for example, the data contains groups of students from the same classroom who have a the same teacher. This can also be the result of clustered sampling, a common practice in multi-stage survey design.\nAssumption CLRM 4. Full rank: \\(rank(X)=k\\quad a.s.\\) a.s.4\nSince \\(X\\) is a random variable we should add to the assumption: \\(rank(X) = k\\) almost surely (abbreviated a.s.). This means that the set of events in which \\(X\\) is not full rank occur with probability 0. The reason for this addition is that such a set of events (in the sample space) may not be empty.\nCLRM 4. is some time referred to as the absence of perfect (or exact) collinearity. Do not confuse this with multicollinearity. Multicollinearity occurs when regressors are highly (linearly) correlated with one another, yielding imprecise estimates.\nAssumption CLRM 5. Normality of the error term: \\(\\varepsilon|X \\sim N(0,\\sigma^2 I_n)\\)\nAssumption CLRM 6. Observations \\(\\{(Y_i,X_i): i=1,...,n\\}\\) are independently and identically distributed (iid).\nCLRM 5 & 6 are not part of the Classical assumptions, but do simplify the problem of inference. Note, CLRM 5 implies independence across error terms, not implied by CLRM 3.\n\n3.1 Non-random \\(X\\)\nThere is an alternative version of the CLRM in which \\(X\\) is a non-random, matrix of regressors/predictors. With \\(X\\) fixed, the error term is the only only random variable in the model. CLRM assumptions 1 and 4 remain the same, while CLRM 2, 3, 5, and 6 become:\nAssumption CLRM 2a. Mean independence of the error term: \\[E[\\varepsilon]=0\\]\nAssumption CLRM 3a. Homoskedasticity: \\(Var(\\varepsilon) = \\sigma^2I_n\\)\nAssumption CLRM 5a. Normality of the error term: \\(\\varepsilon \\sim N(0,\\sigma^2 I_n)\\)\nAssumption CLRM 6a. Observations \\(\\{\\varepsilon_i: i=1,...,n\\}\\) are independently and identically distributed (iid).\n\n\n3.2 Identification\nCLRM 1,2 and 4. are the identifying assumptions of the model. These assumptions allow us to write the parameter of interest as a set of ‚Äòobservable‚Äô moments in the data. We can demonstrate this as follows.\n\nProof. Start with CLRM 2.\n\\[\n    E[\\varepsilon_i|X_i]=0\n\\]\nPre-multiply by the vector \\(X_i\\), \\[\n        X_iE[\\varepsilon_i|X_i]=0\n\\] Since the expectation is conditional on \\(X_i\\), we can bring \\(X_i\\) inside the expectation function,\n\\[\n        E[X_i\\varepsilon_i|X_i]=0\n    \\] This conditional expectation is a random-function of \\(X_i\\). If we take the expectation of this function w.r.t. \\(X\\), we achieve the aforementioned result that conditional mean independence implies zero covariance, \\[\n        E\\left[E[X_i\\varepsilon_i|X_i]\\right]=E[X_i\\varepsilon_i]=0\n    \\]\nNow substitute in for \\(\\varepsilon_i\\) using the linear regression model from CLRM 1 and separate the resulting two terms,\n\\[\n\\begin{aligned}\n    &E[X_i(Y_i-X_i'\\beta)]=0 \\\\\n    \\Rightarrow &E[X_iX_i']\\beta=E[X_iY_i]\n\\end{aligned}\n\\]\nSince \\(\\beta\\) is a non-random vector, we can remove it from the expectation function.\nNow we have a system of linear equations (of the form \\(Av = b\\)) with a unique solution if and only if the matrix \\(E[X_iX_i']\\) is invertible. For the inverse of \\(E[X_iX_i']\\) to exist, we require CLRM 4, since \\(rank(X)=k\\quad a.s.\\Rightarrow rank(E[X_iX_i'])=k\\).5\n\\[\n    \\beta = E[X_iX_i']^{-1}E[X_iY_i]\n\\]\n\nWe cannot compute \\(\\beta\\) because we do not know the joint distribution of \\((Y_i,X_i)\\) needed to solve for the variance-covariance matrices. However, \\(\\beta\\) is (point) identified because both \\(Y\\) and \\(X\\) are observed in the data and the parameters are ‚Äúpinned down‚Äù by a unique set of ‚Äòobservable‚Äô moments in the data.\n\\(\\beta\\) is not identified if the above system of linear equations does not have a unique solution. This will occur if two or more of the regressors are perfectly colinear.6 \\(\\beta\\) is also not be identified if the resulting expression for \\(\\beta\\) includes ‚Äòobjects‚Äô (moments, distribution/scale parameters) that are not ‚Äòobserved‚Äô in the data. For example, if the error is not mean independent, the above expression will include a bias term that depends on \\(E[X_i'\\varepsilon_i]\\).\nIn this instance, the identification of \\(\\beta\\) is scale dependent. That is, if we multiply \\(Y_i\\) by a scalar, \\(\\beta\\) is multiplied by the same scalar. For example, in cases where a researcher is modelling standardized test-scores."
  },
  {
    "objectID": "handout-1neil.html#interpretation",
    "href": "handout-1neil.html#interpretation",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "4 Interpretation",
    "text": "4 Interpretation\nIn this linear regression model each slope coefficient has a partial derivative interpretation,\n\\[\n\\beta_j = \\frac{\\partial E[Y_i|X_i]}{\\partial X_{ij}}\n\\] or, as a vector, \\[\n\\beta = \\frac{\\partial E[Y_i|X_i]}{\\partial X_{i}} = \\begin{bmatrix}\\frac{\\partial E[Y_i|X_i]}{\\partial X_{i1}}\\\\ \\vdots \\\\ \\frac{\\partial E[Y_i|X_i]}{\\partial X_{ik}}\\end{bmatrix} = \\begin{bmatrix}\\beta_1\\\\ \\vdots \\\\ \\beta_k\\end{bmatrix}\n\\]\nNote, the derivative is expressed in terms of changes in the expected value of \\(Y_i\\) (conditional on \\(X_i\\)), not \\(Y_i\\) itself. This is because \\(Y_i\\) is a random variable, but under CLRM 1 & 2\n\\[\nE[Y_i|X_i] = X_i'\\beta\n\\]\nFor a given value of \\(X_i\\), the above expression is non-random.\nAs \\(\\beta_j\\) is a partial derivative, its interpretation is one that ‚Äúholds fixed‚Äù the value of other regressors (i.e.¬†ceteris paribus). Because of this, many researchers apply the experimental language of control variables when interpretting regression coefficients. However, this is dependent on the assumed linearity of the CEF."
  },
  {
    "objectID": "handout-1neil.html#ordinary-least-squares",
    "href": "handout-1neil.html#ordinary-least-squares",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "5 Ordinary Least Squares",
    "text": "5 Ordinary Least Squares\nOLS is an estimator for \\(\\beta\\). As will become evident in Lecture 3, it is not the only estimator for \\(\\beta\\).\nThe OLS estimator is the solution to,\n\\[\n\\min_b\\;\\sum_{i=1}^n(Y_i-X_i'b)^2\n\\]\nUsing vector notation, we can rewrite this as\n\\[\n\\begin{aligned}\n&\\min_b\\;(Y-Xb)'(Y-Xb)\\\\\n=&\\min_b\\;Y'Y-Y'Xb-b'X'Y+b'X'Xb \\\\\n=&\\min_b\\;Y'Y-2b'X'Y+b'X'Xb\n\\end{aligned}\n\\] From line 2 to 3 we use the fact that \\(Y'Xb\\) is a scalar and therefore symmetric: \\(Y'Xb=b'X'Y\\).7\nDifferentiating the above expression w.r.t. the vector \\(b\\) and setting the first-order conditions to \\(0\\), we find that the following condition must hold for \\(\\hat{\\beta}\\), the solution.\n\\[\n  \\begin{aligned}\n  &0=-2X'Y+2X'X\\hat{\\beta}\n  \\\\ \\Rightarrow& X'X\\hat{\\beta} = X'Y\n  \\end{aligned}\n\\]\n\nHow did we get this result? Deriving the first order conditions requires knowledge of how to solve for the derivative of a scalar respect to a column vector (in this case \\(b\\in R^k\\)). The extra material on Linear Algebra has some notes on vector differentiation.\nWe can ignore the first term \\(Y'Y\\) as it does not depend on \\(b\\). The second term is \\(-2b'X'Y\\). Here we can use the rule that, \\[\n  \\frac{\\partial z'a}{\\partial z} = \\frac{\\partial a'z}{\\partial z} = a\n\\] In this instance, \\(a = X'Y \\in R^k\\). Thus, \\[\n  \\frac{\\partial -2b'X'Y}{\\partial b} = -2\\frac{\\partial b'X'Y}{\\partial b} = -2X'Y\n\\] The third term is \\(b'X'Xb\\). This is what is commonly referred to as a quadratic form: \\(z'Az\\). We know that the derivative of this form is, \\[\n  \\frac{\\partial z'Az}{\\partial z} = Az + A'z\n\\] and if \\(A\\) is symmetric, the result simplies to \\(2Az\\). In this instance, \\(A = X'X\\) is symmetric and the derivative is given by, \\[\n  \\frac{\\partial b'X'Xb}{\\partial b} = 2X'X\n\\]\n\nIn order to solve for \\(\\hat{\\beta}\\) we need to move the \\(X'X\\) term to the right-hand side. If these were scalars we would simply divide both sides by the same constant. However, as \\(X'X\\) is a matrix, division is not possible. Instead, we need to pre-multiply both sides by the inverse of \\(X'X\\): \\((X'X)^{-1}\\). Here‚Äôs the issue: the inverse of a matrix need not exist.\nGiven a square \\(k\\times k\\) matrix \\(A\\), its inverse exists if and only if \\(A\\) is non-singular. For \\(A\\) to be non-singular its rank must have full rank: \\(r(A)=k\\), the number of rows/columns. This means that all \\(k\\) columns/rows must be linearly independent. (See Material on Linear Algebra for a more detailed discussion of all these terms.)\nIn our application, \\(A=X'X\\) and\n\\[ r(X'X) = r(X) = colrank(X)\\leq k \\]\nTo insure that the inverse of \\(X'X\\) exists, \\(X\\) must have full column rank: all column vectors must be linearly independent. In practice, this means that no regressor can be a perfect linear combination of others. However, we have this from\nCLRM 4: \\(rank(X)=k\\)\nYou may know this assumption by another name: the absence of perfect colinearity between regressors.\n\nThe rank condition is the reason we exclude a base category when working with categorical variables.\nRecall, most linear regression models are specified with a constant. Thus, the first column of \\(X\\) is\n\\[ X_1 = \\begin{bmatrix}1 \\\\ 1 \\\\ \\vdots \\\\ 1\\end{bmatrix} \\] a \\(n\\times 1\\) vector vector of \\(1\\)‚Äôs, denoted here as \\(\\ell\\). Suppose you have a categorical - for example, gender in an individual level dataset - that splits the same in two. The categories are assumed to be exhaustive and mutually exclusive. If you create two dummy variables, one for each category,\n\\[ X_2 = \\begin{bmatrix}1 \\\\ \\vdots \\\\1\\\\0\\\\ \\vdots \\\\ 0\\end{bmatrix}\\qquad\\text{and}\\qquad X_3 = \\begin{bmatrix}0 \\\\ \\vdots \\\\0\\\\1\\\\ \\vdots \\\\ 1\\end{bmatrix} \\]\nit is evident that \\(X_2+X_3 = \\ell\\). (Here I have depicted the sample as sorted along these two categories.) If \\(X=[X_1\\;X_2\\;X_3]\\), then it is rank-deficient: \\(r(X) = 2&lt;3\\), since \\(X_3=X_1-X_2\\). Thus, we can only include two of these three regressors. We can even exclude the constant and have \\(X=[X_2\\;X_3]\\).\n\nIf \\(X\\) is full rank, then \\((X'X)^{-1}\\) exists and,\n\\[\n\\hat{\\beta} = (X'X)^{-1}X'Y\n\\]\nThis relatively simple expression is the solution to least squares minimization problem. Just think, it would take less than three lines of code to programme this. That is the power of knowing a little linear algebra.\nWe can write the same expression in terms of summations over unit-level observations,\n\\[\n\\hat{\\beta} = \\bigg(\\sum_{i=1}^nX_iX_i'\\bigg)^{-1}\\sum_{i=1}^nX_iY_i\n\\]\nNote, the change in position of the transpose: \\(X_i\\) is a column vector \\(\\Rightarrow\\) \\(X_i'X_i\\) is a scalar while \\(X_iX_i'\\) is a \\(k\\times k\\) matrix. To match the first expression, the term inside the parenthesis must be a \\(k\\times k\\) matrix. Similarly, \\(X'Y\\) is a \\(k\\times 1\\) vector, as is \\(X_iY_i\\).\n\n5.1 Univariate case\nUndergraduate textbooks all teach a very similar expression for the OLS estimator of a uni-variate regression model (with a constant); typically, something like,8\n\\[\nY_i = \\beta_1+\\beta_2X_{i2}+\\varepsilon_i\n\\] We know that the OLS estimators are give by,\n\\[\n\\begin{aligned}\n\\tilde{\\beta}_2 =& \\frac{\\sum(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2)}{\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2} \\\\\n\\text{and}\\qquad \\tilde{\\beta}_1 =& \\bar{Y}-\\tilde{\\beta_2}\\bar{X}_2\n\\end{aligned}\n\\]\nI am deliberately using the notation \\(\\tilde{\\beta}\\) to distinguish these two estimators from the expression below. Let us see if we can replicate this result, using vector notation. The the model is,\n\\[\n\\begin{aligned}\nY =& X\\beta+\\varepsilon \\\\\n=& \\begin{bmatrix}1&X_{12} \\\\ 1 & X_{22} \\\\ \\vdots & \\vdots \\\\ 1 & X_{n2}\\end{bmatrix}\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\end{bmatrix} + \\varepsilon \\\\\n=& \\begin{bmatrix}\\ell &X_{2} \\end{bmatrix}\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\end{bmatrix} + \\varepsilon\n\\end{aligned}\n\\]\nTherefore,\n\\[\n\\begin{aligned}\n\\hat{\\beta} = \\begin{bmatrix}\\hat{\\beta}_1 \\\\ \\hat{\\beta}_2\\end{bmatrix}=&(X'X)^{-1}X'Y \\\\\n=&\\bigg(\\begin{bmatrix}\\ell' \\\\ X_{2}' \\end{bmatrix}\\begin{bmatrix}\\ell &X_{2}\\end{bmatrix}\\bigg)^{-1}\\begin{bmatrix}\\ell'  \\\\ X_{2}' \\end{bmatrix}Y \\\\\n=&\\begin{bmatrix}\\ell'\\ell & \\ell'X_2 \\\\ X_{2}'\\ell & X_2'X_2 \\end{bmatrix}^{-1}\\begin{bmatrix}\\ell'Y  \\\\ X_{2}'Y \\end{bmatrix}\n\\end{aligned}\n\\]\nI went through this rather quickly, using a number of linear algebra rules that you may not be familiar with. Do not worry, the point of the exercise is not become a linear algebra master, but instead to focus on the element of each of each matrix/vector. Each element is a scalar (size \\(1\\times 1\\)).\nIf we right them each down as sums you they might be a little more familiar. First consider the \\(2\\times 2\\) matrix:\n\nelement [1,1]: \\(\\ell'\\ell = \\sum_{i=1}^n 1 = n\\)\nelement [1,2]: \\(\\ell'X_2 = \\sum_{i=1}^nX_{i2} = n\\bar{X}_2\\)\nelement [2,1]: \\(X_2'\\ell = \\sum_{i=1}^nX_{i2} = n\\bar{X}_2\\) (as above, since scalars are symmetric)\nelement [2,2]: \\(X_2'X_2=\\sum_{i=1}^nX_{i2}^2\\)\n\nNext, consider the final \\(2\\times 1\\) vector,\n\nelement [1,1]: \\(\\ell'Y = \\sum_{i=1}^n Y_i = n\\bar{Y}\\)\nelement [2,1]: \\(X_2'Y = \\sum_{i=1}^nY_iX_{i2}\\)\n\nOur OLS estimator is therefore,\n\\[\n\\hat{\\beta} = \\begin{bmatrix} n & n\\bar{X}_2 \\\\ n\\bar{X}_2 & \\sum_{i=1}^nX_{i2}^2 \\end{bmatrix}^{-1}\\begin{bmatrix}n\\bar{Y}  \\\\ \\sum_{i=1}^nY_iX_{i2} \\end{bmatrix}\n\\]\nWe now need to solve for the inverse of the \\(2\\times 2\\) matrix. You can easily find notes on how to do this online. Here, I will just provide the solution.\n\\[\n\\hat{\\beta} = \\frac{1}{n\\sum_{i=1}^nX_{i2}^2-n^2\\bar{X}_2^2}\\begin{bmatrix} \\sum_{i=1}^nX_{i2}^2 & -n\\bar{X}_2 \\\\ -n\\bar{X}_2 &  n\\end{bmatrix}\\begin{bmatrix}n\\bar{Y}  \\\\ \\sum_{i=1}^nY_iX_{i2} \\end{bmatrix}\n\\]\nRemember, this is still a \\(2\\times 1\\) vector. We can now solve for the final solution:\n\\[\n\\begin{aligned}\n\\hat{\\beta} =& \\frac{1}{n\\sum_{i=1}^nX_{i2}^2-n^2\\bar{X}_2^2}\\begin{bmatrix} n\\bar{Y}\\sum_{i=1}^nX_{i2}^2 -n\\bar{X}_2\\sum_{i=1}^nY_iX_{i2} \\\\ n\\sum_{i=1}^nY_iX_{i2}-n^2\\bar{X}_2\\bar{Y}\\end{bmatrix} \\\\\n=& \\frac{1}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2}\\begin{bmatrix} n\\bar{Y}\\sum_{i=1}^nX_{i2}^2 + n^2\\bar{Y}\\bar{X}^2 - n^2\\bar{Y}\\bar{X}^2 -n\\bar{X}_2\\sum_{i=1}^nY_iX_{i2} \\\\ n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2) \\end{bmatrix} \\\\\n=& \\frac{1}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2}\\begin{bmatrix} n\\bar{Y}\\sum_{i=1}^n(X_{i2}-\\bar{X})^2 -n\\bar{X}_2\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2) \\\\ n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2) \\end{bmatrix} \\\\\n=& \\begin{bmatrix} \\bar{Y}  -\\frac{n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2)}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2}\\bar{X}_2 \\\\ \\frac{n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2)}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2} \\end{bmatrix} \\\\\n=& \\begin{bmatrix} \\bar{Y}  -\\tilde{\\beta}_2\\bar{X}_2 \\\\ \\tilde{\\beta}_2 \\end{bmatrix} \\\\\n=& \\begin{bmatrix}\\tilde{\\beta}_1 \\\\ \\tilde{\\beta}_2 \\end{bmatrix}\n\\end{aligned}\n\\]\nThe math is a little involved, but it shows you these solutions are are the same. Unfortunately, the working gets even more arduous in a multivariate context. However, there are useful tools to help us with that we will discuss next.\n\n\n5.2 Geometry of OLS\nIn the last section we saw how the OLS estimator can, more generally, be described as a linear transformation of the \\(Y\\) vector.\n\\[\n\\hat{\\beta} = (X'X)^{-1}X'Y\n\\]\nWe also saw that in order for there to be a (unique) solution to the least squared problem, the \\(X\\) matrix must be full rank. This rules out any perfect colinearity between columns (i.e.¬†regressors) in the \\(X\\) matrix, including the constant.\nGiven the vector of OLS coefficients, we can also estimate the residual,\n\\[\n\\begin{aligned}\n\\hat{\\varepsilon} =& Y - X\\hat{\\beta} \\\\\n=&Y-X(X'X)^{-1}X'Y \\\\\n=&(I_n-X(X'X)^{-1}X)Y\n\\end{aligned}\n\\]\nby plugging the definition of \\(\\hat{\\beta}\\). Thus, the OLS estimator separates the vector \\(Y\\) into two components:\n\\[\n\\begin{aligned}\nY =& X\\hat{\\beta} + \\hat{\\varepsilon} \\\\\n=&\\underbrace{X(X'X)^{-1}X'}_{P_X}Y + (\\underbrace{I_n-X(X'X)^{-1}X}_{I_n-P_X = M_X})Y \\\\\n=&P_XY + M_XY\n\\end{aligned}\n\\]\nThe matrix \\(P_X = X(X'X)^{-1}X'\\) is a \\(n\\times n\\) projection matrix. It is a linear transformation that projects any vector into the span of \\(X\\): \\(S(X)\\subset\\mathbb{R}^n\\). (See for more information on these terms.) \\(S(X)\\) is the vector space spanned by the columns of \\(X\\). The dimensions of this vector space depends on the rank of \\(P_X\\),\n\\[\ndim(S(X)) = r(P_X) = r(X) = k\n\\]\nThe matrix \\(M_X = I_n-X(X'X)^{-1}X'\\) is also a \\(n\\times n\\) projection matrix. It projects any vector into \\(X\\)‚Äôs orthogonal span: \\(S^{\\perp}(X)\\). Any vector \\(z\\in S^{\\perp}(X)\\) is orthogonal to \\(X\\). This includes the estimated residual, which is by definition orthogonal to the predicted values and, indeed, any column of \\(X\\) (i.e.¬†any regressor). The dimension of this orthogonal vector space depends on the rank of \\(M_X\\),\n\\[\ndim(S^{\\perp}(X)) = r(M_X) = r(I_n)-r(X) = n-k\n\\]\nThe orthogonality of these two projections can be easily shown, since projection matrices are idempotent (\\(P_XP_X = P_X\\)) and symmetric (\\(P_X' = P_X\\)). Consider the inner product of these two projections,\n\\[\nP_X'M_X = P_X(I_n-P_X) = P_X-P_XP_X = P_X-P_X = 0\n\\]\nThe least squares estimator is a projection of Y into two vector spaces: one the span of the columns of \\(X\\) and the other a space orthogonal to \\(X\\).\nWhy is this useful? Well, it helps us understand the ‚Äúmechanics‚Äù (technically geometry) of OLS. When working with linear regression models, we typically assume either strict exogeneity - \\(E[\\varepsilon|X]=0\\) - or uncorrelatedness - \\(E[X'\\varepsilon]=0\\) - where the former implies the latter (but not the other way around).\nWhen we use OLS, we estimate the vector \\(\\hat{\\beta}\\) such that,\n\\[\nX'(Y-X\\hat{\\beta})=X'\\hat{\\varepsilon}=0 \\quad always\n\\]\nThis is true, not just in expectation, but by definition. The relationship is ‚Äúmechanical‚Äù: the regressors and estimated residual are perfectly uncorrelated. This can be easily shown:\n\\[\n\\begin{aligned}\nX'\\hat{\\varepsilon} =& X'M_XY \\\\\n=& X'(I_n-P_X)Y \\\\\n=&X'I_nY-X'X(X'X)^{-1}X'Y \\\\\n=&X'Y-X'Y \\\\\n=&0\n\\end{aligned}\n\\]\nYou are essentially imposing the assumption of uncorrelatedness between the explained and unexplained components of Y on the data. This means that if the assumption is wrong, so is the projection.\n\n\n5.3 Partitioned regression\nThe tools of linear algebra can help us better understand partitioned regression. Indeed, I would go as far to to say that it is quite difficult to understand partitioned regression without an understanding of projection matrices. Moreover, we need to understand partitioned regression to really understand multivariate regression. The partitioned regression result is referred to as Frisch-Waugh-Lovell Theorem (FWL).\n\nTheorem 1 FWL says that if you have two sets of regressors, \\([X_1,X_2]\\), then \\(\\hat{\\beta}_1\\), the OLS estimator for \\(\\beta_1\\), from the regression,\n\\[\n        Y = X_1\\beta_1 + X_2\\beta_2 + \\varepsilon\n\\]\nis also given by the regression,\n\\[\n        M_2Y = M_2X_1\\beta_1 + \\xi\n\\]\n\nWe can demonstrate the FWL theorem result using projection matrices. Two simplify matters, we will divide the set of regressors into two groups: \\(X_1\\) a single regressor and \\(X_2\\) a \\(n\\times (k-1)\\) matrix. We can rewrite the linear model as,\n\\[\nY = X\\beta+ \\varepsilon =\\beta_1X_1+X_2\\beta_2+\\varepsilon\n\\]\nLet us begin by applying our existing knowledge. From above, we know that the residual from the regression of \\(X_1\\) on \\(X_2\\) is,\n\\[\n\\hat{\\upsilon} = M_2X_1\n\\]\nwhere \\(M_2 = I_n-X_2(X_2'X_2)^{-1}X_2'\\). It turns out, it does not matter if we residualize \\(Y\\) too. Can you see why? Thus, the model we estimate in the second step, is\n\\[\nY = \\gamma_1\\underbrace{M_2X_1}_{\\hat{\\upsilon}}+\\xi\n\\]\nWe know that \\(\\hat{\\gamma}_1 = (\\hat{\\upsilon}'\\hat{\\upsilon})^{-1}\\hat{\\upsilon}'Y\\). Replacing the value of the residual, we get,\n\\[\n\\begin{aligned}\n\\hat{\\gamma}_1 =& (\\hat{\\upsilon}'\\hat{\\upsilon})^{-1}\\hat{\\upsilon}'Y \\\\\n=&(X_1'M_2M_2X_1)^{-1}X_1'M_2Y \\\\\n=&(X_1'M_2X_1)^{-1}X_1'M_2Y\n\\end{aligned}\n\\] Here, we use both the symmetric and idempotent qualities of \\(M_2\\). Next we want to show that \\(\\hat{\\beta}_1\\) is given by the same value. This part is more complicated. Let‚Äôs start with by reminding ourselves of the following:\n\\[\n\\begin{aligned}\nX'X\\hat{\\beta} &= X'Y \\\\\n\\begin{bmatrix}X_1 & X_2\\end{bmatrix}'\\begin{bmatrix}X_1 & X_2\\end{bmatrix}\\begin{bmatrix}\\hat{\\beta}_1 \\\\ \\hat{\\beta}_2\\end{bmatrix} &= \\begin{bmatrix}X_1 & X_2\\end{bmatrix}'Y \\\\\n\\begin{bmatrix}X_1'X_1 & X_1'X_2 \\\\ X_2'X_1 & X_2'X_2 \\end{bmatrix}\\begin{bmatrix}\\hat{\\beta}_1 \\\\ \\hat{\\beta}_2\\end{bmatrix} &= \\begin{bmatrix}X_1'Y \\\\ X_2'Y\\end{bmatrix}\n\\end{aligned}\n\\]\nWe could solve for \\(\\hat{\\beta}_1\\) by solving for the inverse of \\(X'X\\); however, this will take a long time. An easier approach is to simply verify that \\(\\hat{\\beta}_1=(X_1'M_2X_1)^{-1}X_1'M_2Y\\). Recall, \\(\\hat{\\beta}\\) splits \\(Y\\) into two components:\n\\[\nY = \\hat{\\beta}_1X_1+X_2\\hat{\\beta}_2 + \\hat{\\varepsilon}\n\\]\nIf we plug this definition of \\(Y\\) into the above expression we get,\n\\[\n\\begin{aligned}\n&(X_1'M_2X_1)^{-1}X_1'M_2(\\hat{\\beta}_1X_1+X_2\\hat{\\beta}_2 + \\hat{\\varepsilon}) \\\\\n=&\\hat{\\beta}_1\\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2X_1}_{=I_n} \\\\\n&+\\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2X_2\\hat{\\beta_2}}_{=0} \\\\\n&+\\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2\\hat{\\varepsilon}}_{=0} \\\\\n=&\\hat{\\beta}_1\n\\end{aligned}\n\\]\nIn line 2, I use the fact that \\(\\hat{\\beta}_1\\) is a scalar and can be moved to the front (since the order of multiplication does not matter with a scalar). In line 3, I use the fact that \\(M_2X_2=0\\) by definition. Line 4 uses the fact that \\(M_2\\hat{\\varepsilon}=\\hat{\\varepsilon}\\) which means that \\(X_1'M_2\\hat{\\varepsilon}=X_1'\\hat{\\varepsilon}=0\\).\nThe OLS estimator solves for \\(\\beta_1\\) using the variance in \\(X_1\\) that is orthogonal to \\(X_2\\). This is the manner in which we ‚Äúhold \\(X_2\\) constant‚Äù: the variation in \\(M_2X_1\\) is orthogonal to \\(X_2\\). Changes in \\(M_2X_1\\) are uncorrelated with changes in \\(X_2\\); as if the variation in \\(M_2X_1\\) arose independently of \\(X_2\\). However, uncorrelatedness does NOT imply independence."
  },
  {
    "objectID": "handout-1neil.html#references",
    "href": "handout-1neil.html#references",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "6 References",
    "text": "6 References"
  },
  {
    "objectID": "handout-1neil.html#footnotes",
    "href": "handout-1neil.html#footnotes",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMy notation assumes that \\(X_i\\) is a column vector, which makes \\(X_i'\\beta\\) a scalar. Wooldridge (2010) uses the notation \\(X_i\\beta\\), implying that \\(X_i\\) is a row vector. This is a matter of preference.‚Ü©Ô∏é\nYou might also refer to the vector of regressors or explanatory variables. The terms covariates or control variables are more common in Microeconometrics literature, where regressors are typically included for identification of causal effects. Some texts will use the term independent variables, but this name implies a specific relationship between \\(Y\\) and \\(X\\) that need not hold. Note, we will asssume in this term that \\(n&gt;k\\); i.e.¬†this is ‚Äúsmall‚Äù data.‚Ü©Ô∏é\nThis is NOT the residual.‚Ü©Ô∏é\nSee extra material on Linear Algebra to read more on rank.‚Ü©Ô∏é\nFor \\(n\\) large, \\(rank(E[X_iX_i'])=k\\Rightarrow rank(X)=k\\). This follows from Law of Large Numbers, since \\(plim(n^{-1}X'X) = E[X_iX_i']\\).‚Ü©Ô∏é\nThere are many such failures of (parametric) identification in models that include dummy variables (or fixed effects). Earlier we saw that the intercept is not separately identified from the mean of the error term. Mean independence of the error term, \\(E[\\varepsilon_i]=0\\), is required for us to separately ‚Äòidentify‚Äô \\(\\beta_1\\).‚Ü©Ô∏é\nWhen working with vectors and matrices it is important to keep track of their size. You can only multiply two matrices/vectors if their column and row dimensions match. For example, if \\(A\\) and \\(B\\) are both \\(n\\times k\\) matrices (\\(n\\neq k\\)), then \\(AB\\) is not defined since \\(A\\) has \\(k\\) columns and \\(B\\) \\(n\\) rows. For the same reason \\(BA\\) is also not defined. However, you can pre-multiply \\(B\\) with \\(A'\\) as \\(A'\\) is a \\(k\\times n\\) matrix: \\(A'B\\) is therefore a \\((k\\times n)\\cdot (n\\times k)=k\\times k\\) matrix. Similarly, \\(B'A\\) is defined, but is a \\(n\\times n\\) matrix.\nOrder matters when working with matrices and vectors. Pre-multiplication and post-multiplication are not the same thing.\nKeep track of the size of each term to ensure they correspond to one another. In this instance, each term should be a scalar. For example, \\(-2b'X'Y\\) is the multiplication of a scalar (\\(-2\\): size \\(1\\times 1\\)), row vector (\\(b'\\): size \\(1\\times k\\)), matrix (\\(X'\\): size \\(k\\times n\\)), and column vector (\\(Y\\): size \\(n\\times 1\\)). Thus we have a \\((1\\times 1)\\cdot (1\\times k)\\cdot (k\\times n)\\cdot (n\\times 1)=1\\times 1\\).‚Ü©Ô∏é\nOnce you are familiar with vector notation, it is relatively easy to tell whether a model is uni- or multi-variate. This is because the notation \\(\\beta_2 X_{i2}\\) is not consistent with \\(X_{2i}\\) being a vector (row or column).\nIf \\(X_{i2}\\) is a \\(k\\times 1\\) vector then so is \\(\\beta_2\\). Thus, \\(\\beta_2 X_{i2}\\) is \\((k\\times 1)\\cdot (k\\times1)\\), which is not defined.\nIf \\(X_{i2}\\) is a row vector (as in Wooldridge, 2011), \\(\\beta_2 X_{i2}\\) will then be \\((k\\times 1)\\cdot (1\\times k)\\), a \\(k\\times k\\) matrix. This cannot be correct since the model is defined at the unit level.\nThus, if you see a model written with the parameter in front of the regressor, you know that this must be a single regressor. This is subtle, yet imporant, distinction that researchers often use to convey the structure of their model. Whenever \\(X_{i2}\\) is a vector, researchers will almost always use the notation \\(X_{i2}'\\beta\\) or \\(X_{i2}\\beta\\), depending on whether \\(X_{i2}\\) is assumed to be a column or row vector.‚Ü©Ô∏é"
  },
  {
    "objectID": "week01_advanced_interactive.html",
    "href": "week01_advanced_interactive.html",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "",
    "text": "Note\n\n\n\nDownload PDF"
  },
  {
    "objectID": "week01_advanced_interactive.html#readings",
    "href": "week01_advanced_interactive.html#readings",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "1.1 Readings",
    "text": "1.1 Readings\n\nStock and Watson (2003), Chapters 4‚Äì5\nDougherty (2016), Chapter 2\nWooldridge (2013), Chapter 2"
  },
  {
    "objectID": "week01_advanced_interactive.html#correlation",
    "href": "week01_advanced_interactive.html#correlation",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "3.1 Correlation",
    "text": "3.1 Correlation\nIn economics we are interested in the relationship between two or more random variables, for example:\n\nSales and advertising expenditure\nPersonal consumption and disposable income\nInvestment and interest rates\nEarnings and schooling\n\nA measure of linear association between two random variables \\(x\\) and \\(y\\) is the covariance, which for a sample of \\(n\\) pairs of observations \\((x_1, y_1)\\), \\(\\ldots\\), \\((x_n, y_n)\\) is calculated as\n\\[\n\\operatorname{cov}(x,y)\n=\n\\frac{1}{n-1}\n\\sum_{i=1}^{n}\n(x_i - \\bar{x})(y_i - \\bar{y})\n\\]\nThe covariance is a measure of linear association. It may be approximately zero even when there is a strong non-linear (e.g.¬†quadratic) relationship."
  },
  {
    "objectID": "week01_advanced_interactive.html#interactive-correlation-explorer",
    "href": "week01_advanced_interactive.html#interactive-correlation-explorer",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "3.2 Interactive Correlation Explorer",
    "text": "3.2 Interactive Correlation Explorer\n\n\n\n\n\n\nExplore Correlation Interactively!\n\n\n\nAdjust the sliders below to see how different parameters affect the relationship between variables. This hands-on exploration helps build intuition for regression concepts.\n\n\n\n\n\n\nüéÆ Control Panel\n\n\nüìä Correlation (œÅ)\n\n0.7\n\n\nüë• Sample Size (n)\n\n100\n\n\nüîä Noise Level (œÉ)\n\n0.5\n\n\n\nüé≤ Regenerate Data\nüìà Show Regression Stats\n\n\n\n\nüìä Regression Statistics\n\n\n\n\n\n\n\n\n\n\n\n\nüéì Learning Objectives\n\n\n\n\nExperiment with correlation strength to see how it affects scatter patterns\nObserve how sample size influences the precision of estimates\nUnderstand the role of noise (random error) in obscuring relationships\nCompare the actual correlation with the estimated R-squared value"
  },
  {
    "objectID": "week01_advanced_interactive.html#understanding-different-correlation-patterns",
    "href": "week01_advanced_interactive.html#understanding-different-correlation-patterns",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "3.3 Understanding Different Correlation Patterns",
    "text": "3.3 Understanding Different Correlation Patterns"
  },
  {
    "objectID": "week01_advanced_interactive.html#overview",
    "href": "week01_advanced_interactive.html#overview",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "3.4 Overview",
    "text": "3.4 Overview\nIn this handout we will revisit the Classical Linear Regression Model (CLRM) [see @wooldridge2010, chap. 1-2]. The goal of this week‚Äôs lecture is to:\n\nunderstand the model specification;\nits underlying assumptions;\nand the appropriate interpretation;\nthe OLS estimator, using linear algebra;\nthe geometry of OLS and partitioned regression result."
  },
  {
    "objectID": "week01_advanced_interactive.html#model-specification",
    "href": "week01_advanced_interactive.html#model-specification",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "3.5 Model Specification",
    "text": "3.5 Model Specification\nThe linear population regression model is given by,\n\\[\n\\begin{aligned}\n  Y_i =& X_i'\\beta+\\varepsilon_i \\\\\n  =& \\beta_1\\mathbf{1}+\\beta_2X_{i2}+\\beta_3X_{i3}+...+\\beta_kX_{ik}+\\varepsilon_i\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "week01_FINAL.html",
    "href": "week01_FINAL.html",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "",
    "text": "Note\n\n\n\nDownload PDF"
  },
  {
    "objectID": "week01_FINAL.html#readings",
    "href": "week01_FINAL.html#readings",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "1.1 Readings",
    "text": "1.1 Readings\n\nStock and Watson (2003), Chapters 4‚Äì5\nDougherty (2016), Chapter 2\nWooldridge (2013), Chapter 2"
  },
  {
    "objectID": "week01_FINAL.html#correlation",
    "href": "week01_FINAL.html#correlation",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "3.1 Correlation",
    "text": "3.1 Correlation\nIn economics we are interested in the relationship between two or more random variables, for example:\n\nSales and advertising expenditure\nPersonal consumption and disposable income\nInvestment and interest rates\nEarnings and schooling\n\nA measure of linear association between two random variables \\(x\\) and \\(y\\) is the covariance, which for a sample of \\(n\\) pairs of observations \\((x_1, y_1)\\), \\(\\ldots\\), \\((x_n, y_n)\\) is calculated as\n\\[\n\\operatorname{cov}(x,y)\n=\n\\frac{1}{n-1}\n\\sum_{i=1}^{n}\n(x_i - \\bar{x})(y_i - \\bar{y})\n\\]\nThe covariance is a measure of linear association. It may be approximately zero even when there is a strong non-linear (e.g.¬†quadratic) relationship."
  },
  {
    "objectID": "week01_FINAL.html#interactive-correlation-explorer",
    "href": "week01_FINAL.html#interactive-correlation-explorer",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "3.2 Interactive Correlation Explorer",
    "text": "3.2 Interactive Correlation Explorer\n\n\n\n\n\n\nExplore Correlation Interactively!\n\n\n\nAdjust the sliders below to see how different parameters affect the relationship between variables. This hands-on exploration helps build intuition for regression concepts.\n\n\n\n  \n  \n    \n    üéÆ Control Panel\n    \n    \n      \n      \n        üìä Correlation (œÅ)\n        \n        0.7\n      \n      \n      \n        üë• Sample Size (n)\n        \n        100\n      \n      \n      \n        üîä Noise Level (œÉ)\n        \n        0.5\n      \n    \n    \n    \n      \n        üé≤ Regenerate Data\n      \n      \n        üìà Show Regression Stats\n      \n    \n  \n  \n  \n  \n  \n    üìä Regression Statistics\n    \n  \n\n\n\n\n\n\n\n\n\n\nüéì Learning Objectives\n\n\n\n\nExperiment with correlation strength to see how it affects scatter patterns\nObserve how sample size influences the precision of estimates\nUnderstand the role of noise (random error) in obscuring relationships\nCompare the actual correlation with the estimated R-squared value"
  },
  {
    "objectID": "week01_FINAL.html#understanding-different-correlation-patterns",
    "href": "week01_FINAL.html#understanding-different-correlation-patterns",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "3.3 Understanding Different Correlation Patterns",
    "text": "3.3 Understanding Different Correlation Patterns"
  },
  {
    "objectID": "week01_FINAL.html#overview",
    "href": "week01_FINAL.html#overview",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "3.4 Overview",
    "text": "3.4 Overview\nIn this handout we will revisit the Classical Linear Regression Model (CLRM) [see @wooldridge2010, chap. 1-2]. The goal of this week‚Äôs lecture is to:\n\nunderstand the model specification;\nits underlying assumptions;\nand the appropriate interpretation;\nthe OLS estimator, using linear algebra;\nthe geometry of OLS and partitioned regression result."
  },
  {
    "objectID": "week01_FINAL.html#model-specification",
    "href": "week01_FINAL.html#model-specification",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "3.5 Model Specification",
    "text": "3.5 Model Specification\nThe linear population regression model is given by,\n\\[\n\\begin{aligned}\n  Y_i =& X_i'\\beta+\\varepsilon_i \\\\\n  =& \\beta_1\\mathbf{1}+\\beta_2X_{i2}+\\beta_3X_{i3}+...+\\beta_kX_{ik}+\\varepsilon_i\n\\end{aligned}\n\\]"
  }
]